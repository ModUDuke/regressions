--- 
title       : "Regressions 1: Introduction to Regression As Causality"
description : "This chapter will introduce you to using regression analysis to find causal effects"
 
 
  
--- type:VideoExercise lang:r aspect_ratio:62.5 xp:50 skills:1 key:5c9d6dd3fc
## Introduction to Regression Analysis
*** =video_link
//player.vimeo.com/video/217553687



--- type:MultipleChoiceExercise lang:r xp:50 skills:1 key:1a2137dd19
## Interpreting Regressions
The popular online auctioneer, eGulf, wants to create an algorithm that will automatically provide recommended sales prices to eGulf sellers. In the case of wePhones, eGulf plans to base its algorithm on the age and sales price of previously sold WePhones. For the sake of convenience, eGulf bases their algorithm on a small stratified sample of 25 WePhones sold in the past week. 

eGulf runs an OLS regression to determine the relationship between WePhone age and sales price, and plots their results (illustrated in the R workspace). Each point on the plot indicates the age and selling price for a particular WePhone, and the line shows the results from an OLS regression on this data. What do these results suggest about the relationship of WePhone age and sales price?


*** =instructions
- There is a negative relationship
- There is no relationship
- There is a positive relationship
*** =pre_exercise_code
```{r}
set.seed(1)
library(ggplot2)
WePhone<-data.frame(age=rep(c(1,2,3,4,5),5))
WePhone$Feedback<-round(rnorm(n=25,mean=90,sd=3))
WePhone$Age<-WePhone$age+rnorm(n=25,mean=0,sd=.3)-WePhone$Feedback/10+mean(WePhone$Feedback/10)
WePhone$Value<-500-(9*WePhone$age^2)+ round(rnorm(n=25,mean=0,sd=30))
names(WePhone)[4]<-"Price Sold"
WePhone$`Price Sold`<-WePhone$`Price Sold`+20*(WePhone$Feedback-mean(WePhone$Feedback))
model<-lm(`Price Sold`~Age,data=WePhone)
ggplot(data=WePhone,aes(Age, `Price Sold`))+geom_point()+geom_abline(intercept = model$coefficients[1],slope=model$coefficients[2])+ ggtitle("Scatter Plot and OLS Regression of WePhone Age on Price Sold")
```
*** =sct
```{r}
msg1 = "Correct! As the age of WePhones increased, the price at which they were sold decreased."
msg2 = "Although the plot does not indicate if their results were statistically significant, they do indicate a relationship. Try again"
msg3 = "Think this through carefully. Does an increase in age increase or decrease the price sold of a WePhone? Try again"
test_mc(correct = 1, feedback_msgs = c(msg1,msg2,msg3))
```



--- type:MultipleChoiceExercise lang:r xp:50 skills:1 key:4b9a40fa01
## Reversing Causal Direction
Causal interpretation of regression models can be tricky. Using the same data in the last example, an eGulf analyst decided to toy with a regression model where age is a function of price sold (i.e. price is the independent variable, and age is the dependent variable). The results of this model are visualized in the R workspace. If this model shows a causal relationship between age and price sold, with age as the dependent variable, what **exactly** should the analyst say to describe the results to his boss? 

*** =instructions
- Older phones tend to sell for less
- Old age causes phones to sell for less
- High sales prices causes phones to become younger
*** =pre_exercise_code
```{r}
set.seed(1)
library(ggplot2)
WePhone<-data.frame(age=rep(c(1,2,3,4,5),5))
WePhone$Feedback<-round(rnorm(n=25,mean=90,sd=3))
WePhone$Age<-WePhone$age+rnorm(n=25,mean=0,sd=.3)-WePhone$Feedback/10+mean(WePhone$Feedback/10)
WePhone$Value<-500-(9*WePhone$age^2)+ round(rnorm(n=25,mean=0,sd=30))
names(WePhone)[4]<-"Price Sold"
WePhone$`Price Sold`<-WePhone$`Price Sold`+20*(WePhone$Feedback-mean(WePhone$Feedback))
model<-lm(Age~`Price Sold`,data=WePhone)
ggplot(data=WePhone,aes(`Price Sold`,Age))+geom_point()+geom_abline(intercept = model$coefficients[1],slope=model$coefficients[2])+ ggtitle("Scatter Plot and OLS Regression of WePhone Age on Price Sold")
```
*** =sct
```{r}
msg1 = "This phrasing does not address a causal interpretation of the relationship, but rather is a description of an association between WePhone age and sales price"
msg2 = "This is the most intuitive explanation for the relationship between WePhone sale price and age, but not for this particular regression model, which was designed to use age as the dependent variable. Try again"
msg3 = "Correct! Well, at least according to this model. This is an example of reverse causality. Obviously, we know that in reality the age of any object can only be influenced by time, so we know that this interpretation of their causal relationship is false: price sold cannot cause a phone's age. The analyst should try a different model next time."
test_mc(correct = 3, feedback_msgs = c(msg1,msg2,msg3))
```




--- type:MultipleChoiceExercise lang:r xp:50 skills:1 key:d50636b4ac
## Regression Models and Policy Implementation
Suppose you see a Facebook post with a regression plot of the Number of Walmarts per Capita on Income per Capita, and the slope is positive. If you were asked to advise a town on a policy to attract a Walmart to town, would you say they should give tax refunds to their citizens to increase their income?
*** =instructions
- Yes, the regression shows that Walmarts come to richer places
- No, you can't learn the cause of Walmart locations from the regression
*** =sct
```{r}
msg1 = "Whoops. Try again."
msg2 = "No, you can't learn the direct causal effect from this regression as described, because there are so many other social and economic factors to consider in this relationship that Unconfoundness Assumption is unlikely to be true. In this case, we can just learn the amount of correlation between the variables."
test_mc(correct = 2, feedback_msgs = c(msg1,msg2))
```


--- type:VideoExercise lang:r aspect_ratio:62.5 xp:50 skills:1 key:8bb34ee718
## Basic Elements of a Regression Table
*** =video_link
//player.vimeo.com/video/217554002


--- type:MultipleChoiceExercise lang:r xp:50 skills:1 key:470a340b81
## Reading a Bivariate Regression Table
The following table shows the regression results from the previous questions about eGulf's pricing algorithm for used wePhones. The coefficient for age reflects the slope in the previous line graph, and it suggests that for each additional year (or unit of age), the expected sales price of WePhones decreases by $61.99. The coefficient for the intercept matches the Y-axis intercept in the previous line graph. It indicates the predicted price sold for a WePhone at 0 years old (or more generally, it shows the value of an outcome when all other independent variables are at their reference point). 

| Variable   | Coefficient (SE) |
|------------|-----------------:|
|    Age     |   -61.99 (7.28)  |
| Intercept  |   592.56 (24.42) |

To determine the predicted sales price of a WePhone with a given age, we can start with the initial intercept value and add to it the total change in price for the age of the phone. In other words, based on this regression model the formula for predicting the price of a WePhone sold on eGulf is:

`Sales Price = 592.56 + Age * -61.99`

Based on this formula, what is a valid predicted price for a WePhone that is exactly two years old?



*** =instructions
- 460.49
- 468.58
- 472.57 
- 475.56
*** =sct
```{r}
msg1 = "Try again"
msg2 = "Well done! Notice any advantages of regression models versus t.tests? In a t.test, we can only predict the outcome from a binary treatment variable (that is, the outcome for respondents in the treatment versus control group), In a regression model, we can predict the outcome from a continuous treatment variable (e.g. age)."
msg3 = "Try again"
msg4 = "Try again"
test_mc(correct = 3, feedback_msgs = c(msg1,msg2,msg3,msg4))
```



--- type:MultipleChoiceExercise lang:r xp:50 skills:1 key:9917eaebf2
## Reading a Multivariate (Multiple) Regression Table
In our previous eGulf example, we assumed that age is the only determinant of a WePhone's sales price. However, sellers on eGulf also have "feedback scores," ranging from 0-100, that indicate the quality of their previous transactions with customers. It seems likely that a seller's feedback score could positively impact a WePhone's sales price; that is, customers might be willing to pay more for WePhones sold by reputable sellers. However, if reputable sellers tended to sell newer WePhones, this would confound the relationship between sales price and age. 

We could consider overcoming this by throwing out respondents with high feedback scores that sold new WePhones, so that WePhone age and WePhone feedback score were no longer correlated. That might let us look at the effect of the feedback score in a natural experiment analysis, but throwing out those high scores might imbalance our sample and make that analysis invalid. However, with OLS regression, we can simply add seller feedback scores as another determinant of our independent variable, thereby controlling for its confounding effect on our outcome variable. This is what is shown in the table below.

| Variable   | Coefficient (SE) |
|------------|-----------------:|
|    Age     |  -55.61 (4.94)   |
|  Feedback  |   14.15 (2.55)   |
| Intercept  | -707.23 (234.38) |


As you can see, when controlling for seller score, age has an even larger negative effect on sales. Using this table, calculate the difference (in absolute terms) in the expected sales price for phone that is 2 year old with a feedback score of 80, versus a phone that is 4 years old with a feedback score of 90.


*** =instructions
- 22.85
- 26.83 
- 27.00
- 30.28
*** =sct
```{r}
msg1 = "Try again"
msg2 = "Try again"
msg3 = "Try again"
msg4 = "Well done! The same method can applied with as many variables as we choose. This provides a great way of examining the trade offs between different factors that effect on outcome. However, notice that the intercept is -707.23. That would indicate that a used WePhone with an age of 0 and a feedback score of 0 would have a predicted sales price of -$707.23. Obviously that is unrealistic; no sales on eGulf result in sellers having to pay their customers. This hints at a modeling issue that we'll explore later in the chapter."
test_mc(correct = 4, feedback_msgs = c(msg1,msg2,msg3,msg4))
```


--- type:MultipleChoiceExercise lang:r xp:50 skills:1 
## Coefficients of Zero
Does a regression coefficient of 0 with a small confidence interval / small standard errors imply that the outcome and treatment variables simultaneously cause each other?
*** =instructions
- Yes, it shows the variables cancel each other out.
- No, it just shows there's no relationship between these variables in the data.
- It shows "misspecification" and we can't learn anything.

*** =sct
```{r}
msg1 = "It is possible to have simultaneous causality and yet find a regression coefficient of zero, but this is not commonly the reason we see regressions of zero, nor something we could reasonably infer from a coefficient of 0"
msg2 = "Correct! Regressions by themselves are about describing relationships. So a zero regression coefficient shows that there is no relationship in the data between these variables. It does *not* say anything about the causal effect of one variable on the other, unless we make additional assumption, like unconfoundedness."
msg3 = "Not necessarily. We may see a zero coefficient because the model is misspecified, but should not be our immediate conclusion. Moreover, there's almost always something that we can learn from a statistical model; at the very leasy, a statitical model can teach us that we picked the wrong statistical model for our assessing our data."
test_mc(correct = 2, feedback_msgs = c(msg1,msg2,msg3))
```



--- type:VideoExercise lang:r aspect_ratio:62.5 xp:50 skills:1 key:ffa5133d86
## The Relationship between Economic Development and Property Rights
*** =video_link
//player.vimeo.com/video/217554163




--- type:MultipleChoiceExercise lang:r xp:50 skills:1 key:fd77ad9990
## Regressions with small coefficients and small confidence intervals
Does a regression coefficient of 0 with a small confidence interval / small standard errors imply that the outcome and treatment variables simultaneously cause each other?
*** =instructions
- Yes, it shows the variables cancel each other out.
- No, it just shows there's no relationship between these variables in the data.
- It shows "misspecification" and we can't learn anything
*** =sct
```{r}
msg1 = "Actually, OLS regression models can't show simultaneous causality. More advanced methods are needed for this. Try again."
msg2 = "Regressions by themselves are about describing relationships. So a zero regression coefficient shows that there is no relationship in the data between these variables. It does *not* say anything about the causal effect of one variable on the other, unless we make additional assumption, like unconfoundedness. Without extra assumptions, it is possible to have simultaneous causality and yet find a regression coefficient of zero."
msg3 = "Although misspecification might cause a regression model to have no coefficients for variables that should be associated with the outcome of interest, this is not necessarily the case. Moreover, every regression model can teach you something; at the very least, a misspecified regression models can teach us how *not* to specify a regression model for our outcomes of interest."
test_mc(correct = 2, feedback_msgs = c(msg1,msg2,msg3))
```


--- type:MultipleChoiceExercise lang:r xp:50 skills:1 key:f47fad8c5f
## Recalling terminology
Why do we use the words "left hand side" variable and "right hand side" variable in casual inference?

*** =instructions
- It refers to how you graph them on X and Y axis.
- It comes from historical, more qualitative writings.
- It refers to the left and right hand side of an equation.
*** =sct
```{r}
msg1 = "Good try, but remember that on a graph, the Y axis is usually marked on the left side of the page, and the X axis is usually marked on the *bottom* of the page. Try again."
msg2 = "Statistics is a relatively young and quantitative (almost by definition) field. Try again."
msg3 = "When we write down a regression mathematically as an equation, we typically write it as 'Y=beta*X + U' where Y is our outcome variable and X is our treatment or policy variable. Beta here is the regression coefficient and U represents unobserved variables besides X that also affect Y. So 'left hand side' just refers to the left hand side of the equal sign in this equation, while 'right hand side' refers to the right hand side of the equal sign in this equation."
test_mc(correct = 3, feedback_msgs = c(msg1,msg2,msg3))
```


--- type:MultipleChoiceExercise lang:r xp:50 skills:1 key:408e61980c
## Multiple Regression Models
Papers often report several regression models next to each other. This table shows a few truncated models from the example in the video, about the relationship between a country's property rights protections and its GDP. Model 1 shows the coefficient and standard error for the effect of property rights protection on GDP without any control variables. Model 2 shows the effect of property rights protection on GDP while controlling for the country's latitude, and Model 3 shows the same, while also controlling for whether the country is in Asia. Why might the effect of property rights protections on GDP decrease as we add more variables to our model?   

| Variable   | Model 1 | Model 2 | Model 3 |
|------------|--------:|--------:|---------|
| Protection |.52(.06) |.47(.06) |.43(.05) |
| Latitude   |         |.89(.49) |.37(.51) |
| Asia Dummy |         |         |-.62(.19)|


*** =instructions
- Because the inclusion of more variables weakens the statistical power of an independent variable
- Because omitting these variables caused the effect of property rights protection on GDP to be downwardly biased. 
- Because the inclusion of more variables strengthens the statistical power of an independent variable
- Because omitting these variables caused the effect of property rights protection on GDP to be upwardly biased. 
*** =sct
```{r}
msg1 = "Although this is often true, this is not indicated in this figure. If you look at the standard errors, the statistical significance of property rights protections on GDP is relatively small and consistent across models. Try again."
msg2 = "This would mean that the omitted variables confound the relationship between property rights protections and GDP by decreasing the it. Try again"
msg3 = "This is usually not true, nor shown in this figure. Try again."
msg4 = "Correct! Without controlling for these variables, the relationship between property rights protections and GDP seemed larger than it truly was."
test_mc(correct = 4, feedback_msgs = c(msg1,msg2,msg3,msg4))
```



--- type:NormalExercise lang:r aspect_ratio:62.5 xp:50 skills:1 key:631cb11d99
## Practice Running a Regression Model
Dr. Max Funn is interested in understanding the effect of income on happiness. He has a small sample of the General Social Survey (GSS), which asked respondents about their income and happiness independently. Although Dr. Funn predicts that income is positively associated with happiness, he is worried that the relationship between income and happiness might be confounded by their joint relationship with work hours. Specifically, Dr. Funn believes that people who work more hours make more money (which indirectly increases a person's happiness), but that people who work more hours at the same income tend to be less happy (i.e. work hours has a direct negative effect on happiness). 

Using the dataset, `GSS`, help Dr. Max Funn measure the effect of income on happiness, **controlling** for work hours. Specifically:


*** =instructions
- Construct a regression model that measures the effect of income on happiness.
- Construct a second regression model that measures the effect of income on happiness, controlling for work hours.

*** =pre_exercise_code
```{r}
n=1000
set.seed(1)
#Create rnorm function that allows for min and max
  rtnorm <- function(n, mean, sd, min = -Inf, max = Inf){
      qnorm(runif(n, pnorm(min, mean, sd), pnorm(max, mean, sd)), mean, sd)
  }
#Create rounding function that allows to round to numbers above 1
  mround <- function(x,base){ 
          base*round(x/base) 
  } 
#Create scaling function that puts number between 0 and 1
  scale <- function(x){
    (x - min(x))/(max(x)-min(x))
    }

#Dataframe
  GSS<-data.frame(WorkHours=1:n,Income=1:n,Happiness=1:n)
#Work hours 
  GSS$WorkHours<-round(rtnorm(n=n,mean=40,sd=6,min=0,max=55))
#Income (in thousands)
  GSS$Income<-GSS$WorkHours*.5+mround(rtnorm(n=n,mean=30,sd=5,min=0,max=100),.1)
#Scaled variables
  scaledincome<-scale(GSS$Income)
  scaledWorkHours<-scale(GSS$WorkHours)
#Happiness (set as 1:5; rounded to 1; make income 2* as effective as work hours)
  GSS$Happiness<-round(scaledincome-(.5*scaledWorkHours) + rtnorm(n=n,mean=3,sd=.5,min=0,max=5),0)
  
  summary(GSS$WorkHours)
  summary(GSS$Income)
  summary(GSS$Happiness)
    cor(GSS$WorkHours,GSS$Income)
    
```
*** =sample_code
```{r}
# Before running a regression model, let's examine the data
    str(GSS)

# The dataset contains three variables: income (measured in thousands of dollars), work hours, and happiness (measured 1=very unhappy, 2=unhappy, 3=neither happy or unhappy, 4=happy, 5=very happy)
    summary(GSS$Income)
    summary(GSS$WorkHours)
    summary(GSS$Happiness)

# Using the correlation function, we can tell that income and happiness are positively correlated. But by how much? What is the effect of increasing one's income by $1000 on their happiness?
    cor(GSS$Income,GSS$Happiness)
  
# Let's use the glm function to do a linear regress of the effect of income on happiness. For reference, the syntax for glm is "glm(Y~X,data=df)" where Y is the name of the independent variable, X is the name of the dependent variable, and df is the name of the dataframe.

#---- Question 1-------------------------------------#
      Solution1<-glm()
#----------------------------------------------------#

# The glm function in R usually provides less information about the model than desired, and usually in a less than ideal format. Let's use the summary function on our model to see its effects more clearly.
    summary(Solution1)

# The table beneath "Coefficients" is what is usually reported in scientific journals. "Estimate" refers to the coefficients for each parameter, "Pr(>|t|)" refers to the p.value of the estimates, and the stars following p.value refer to the result's statistical significance. 
    
# If you entered Solution1 correctly, the effect of Income should be about .015, meaning that a single unit increase in income ($1000) is associated with a .015 increase in a person's reported happiness. The low p.value and stars to the right of the coefficients suggest that this effect is statistically significant. 

# This relationship can also be viewed graphically via the syntax below. The points on the plot represent each observation, and the diagonal line represents the regression results. Note that the slope of the line is equal to the coefficient for income.
    plot(GSS$Happiness ~ GSS$Income)
    abline(Solution1)

# Now let's move on to Question 2.Let's look at the correlations of work hours on happiness and income. They show that work hours are negatively correlated with happiness and positively correlated with income. This suggests that work hours might confound the relationship between income and happiness.
    cor(GSS$WorkHours,GSS$Happiness)
    cor(GSS$WorkHours,GSS$Income)

# The next best step is to control for the effect of work hours on happiness. You can do this for yourself easily in R by adding the control variable directly to the glm function to regress the effect of income on happiness. For reference, the syntax for glm with controls is "glm(Y~X+Z,data=df)" where Y is the name of the independent variable, X is the name of the dependent variable, Z is the name of the control variable, and df is the dataframe.

#---- Question 2-------------------------------------#
      Solution2<-glm()
#----------------------------------------------------#
  
```
*** =solution
```{r}
    Solution1<-glm(Happiness~Income,data=GSS)
    Solution2<-glm(Happiness~Income+WorkHours,data=GSS)
```
*** =sct
```{r}
test_object("Solution1")
test_object("Solution2")
success_msg("Good work! You may have noticed that the coefficient for income increased when we controlled for WorkHours. This is typical when a control variable is positively correlated with both the independent and dependent variables. This syntax can be used to control for as many variables as desired in a regression model.")
```


--- type:MultipleChoiceExercise lang:r xp:50 skills:1 
## Steps prior to analysis with regression models 
Which of the following is the most important step you should take before running a regression model on your data?
*** =instructions
- Identify how many variables you can include in your model while still attaining statistical significance
- Identify your outcome of interest (i.e. depend variable)
- Identify which type of regression model to perform
- Identify whether any of your variables will need to be transformed
*** =sct
```{r}
msg1 = "This is actually one of the later steps to take when building a regression model. It is usually best to build simple models before adding numerous control variables. Try again."
msg2 = "Correct! This answer may seem obvious, but its importance cannot be understated.  Identifying one's outcome of interest is often the most frequently skipped step among newcomers to statistical analysis. All statistical analysis should be guided by theory about the causal relationship between an outcome of interest and its antecedents. Having a clear plan about what relationship you plan to test is extremely helpful in determining how to specific your statistical model. Conversely, mining through data without any clear theory guiding your research rarely leads to fruitful or interesting findings. "
msg3 = "This is an important step, but it is not the most important of your options. Try again."
msg4 = "Depending on the variables in you plan to use in your model, this is not always an important, much less necessary, step when building your model. Try again."
test_mc(correct = 2, feedback_msgs = c(msg1,msg2,msg3,msg4))
```



--- type:VideoExercise lang:r aspect_ratio:62.5 xp:50 skills:1 key:d75a4ac636
## Using Regression to Get Causal Effects: Unconfoundedness
*** =video_link
//player.vimeo.com/video/217554416



--- type:MultipleChoiceExercise lang:r xp:50 skills:1 key:36681d6cec
## Regressing Soda pop and K-pop
You are studying the effect of pop songs on teenage product consumption using a dataset from South Korea. Via a regression model, you find that for every four songs downloaded, soft drink sales tend to increase by about one can. Does this imply that South Korean Soda Companies should start sponsoring pop stars?
*** =instructions
- No
- Yes
*** =sct
```{r}
msg1 = "Correct. The regression model does not necessarily recover a true causal effect of song downloads on soft drink sales."
msg2 = "Whoops. Try again."
test_mc(correct = 1, feedback_msgs = c(msg1,msg2))
```



--- type:MultipleChoiceExercise lang:r xp:50 skills:1 key:5c60df0d16
## Knowing it all
If you could observe every single variable that mattered for determining outcomes, would you be able to learn all the causal effects you want?
*** =instructions
- No, regressions only give you correlation, not causation.
- Yes, you'd have every combination of variables possibly needed for determining causality.
- Yes, but just for that dataset.
- No, it would give you conflicting information.
*** =sct
```{r}
msg1 = "Although it may seem like that is what this course is implying, there are times when regressions can demonstrate a causal relationship. Try again."
msg2 = "According to our definition of causality, if you truly observed every relevant variable, you would indeed be able to learn all the causal effects you want. There are a few caveats though. First, you would still need to ensure that there was at least some variation in each variable you cared about. Even if you observed every variable that mattered, if you never see those variables change across units, you will not be able to learn their causal effects. Second, no real world dataset in social science will truly have all variables you want, which is why the unconfoundedness assumption is often made-to assume that the variables we do not observe are not related to the treatment."
msg3 = "This answer is tricky. Remember, the dataset contains every relevent variable in the universe for determining outcomes. That should make findings from it generalizable beyond the dataset. Try again."
msg4 = "It's true that you might have a hard time finding statistically significant results if your sample was small, as there theoretically could be an infinite number of relevant variables for any outcome in the universe, many of which are correlated. However, many events do have a variety of causes, and many regression models do incorporate a lot of 'conflicting' variables. Try again."
```



--- type:MultipleChoiceExercise lang:r xp:50 skills:1 key:ff6e381314
## The Uncounfoundedness Assumption
Although regression models are great for summarizing the association between variables, any causal interpretation of a regression models rests on the uncounfoundedness assumption (also known as the selection-on-observables assumption; and sometimes referred to as the conditional independence assumption). Which of the following summarizes this assumption?

*** =instructions
- All variables that affect treatment assignment and the outcome of interest are observable and conditioned upon.
- If something happens more frequently than expected during a given period, we can expect that it will happen less frequently in the future
- There is little or no multicollinearity in the data
- The residuals of a regression model are normally distributed.
*** =sct
```{r}
msg1 = "Correct. Put even more simply, any causal interpretation of a regression model assumes that the relationships observed in the data are unconfounded. In most cases, this is impossible to verify empirically. However, it is a pretty large assumption. Most variations of regression models that we will describe in later chapters try to reduce the strength of this assumption."
msg2 = "This is actually a summary of the gambler's fallacy. Try again"
msg3 = "This is an assumption that regression models make, but it is not a description of the unconfoundedness assumption"
msg4 = "This is an assumption that regression models make, but it is not a description of the unconfoundedness assumption"
test_mc(correct = 1, feedback_msgs = c(msg1,msg2,msg3,msg4))
```



--- type:VideoExercise lang:r aspect_ratio:62.5 xp:50 skills:1 key:9b0c9370cf
## How to Compute Regressions: Ordinary Least Squares (OLS)
*** =video_link
//player.vimeo.com/video/217554577


--- type:NormalExercise lang:r xp:50 skills:1 key:1ff9f6d33f
## Toying with OLS I: Outliers
Although the math behind determining an OLS regression model is complicated, understanding what it's trying to do is surprisingly straightforward. Understanding how regression models work can yield important insight into understanding how they can be biased. In this question, we will examine how outliers can effect regression slopes. Using data provided by the popular online auctioneer eGulf about the relationship between WePhone age and sales price, follow the sample code to answer the following question about outliers:

*** =instructions
- When does an outlier in a regression model's dependent variable, "Y", tend to cause the least bias in its relationship to an independent variable, "X" (i.e. its the regression line's slope)? 
- Answer options: A) When the outlier in Y occurs at low values of X, B) When the outlier in Y occurs at middle values of X, C) When the outlier in Y occurs at high values of X.
*** =pre_exercise_code
```{r}
set.seed(1)
library(ggplot2)
WePhone<-data.frame(age=rep(c(1,2,3,4,5),5))
WePhone$Feedback<-round(rnorm(n=25,mean=90,sd=3))
WePhone$Age<-WePhone$age+rnorm(n=25,mean=0,sd=.3)-WePhone$Feedback/10+mean(WePhone$Feedback/10)
WePhone$Value<-500-(9*WePhone$age^2)+ round(rnorm(n=25,mean=0,sd=30))
names(WePhone)[4]<-"Price Sold"
WePhone$`Price Sold`<-WePhone$`Price Sold`+20*(WePhone$Feedback-mean(WePhone$Feedback))
WePhone$age<-NULL
WePhone$Feedback<-NULL
names(WePhone)[2]<-"PriceSold"
```

*** =sample_code
```{r}
# As a quick refresher, let's glimpse at the first few lines of data. Age refers to the number of years old the WePhone was, and Price Sold refers to the dollars it sold for.
    head(WePhone)

# A regression model between a WePhone's Age and Price Sold produces the following results.
    model<-glm(PriceSold~Age,data=WePhone)
    summary(model)
    
# Below visualizes the regression model as a line, and the data points as dots. The line that it (and any regression model) produces minimizes the vertical distance (squared) between each data point and the regression line.  
    plot(WePhone$PriceSold ~ WePhone$Age)
    abline(model)
    
# What happens when we introduce outliers in the data? The following examples show what happen to the regression line when we insert an observation with an unusally high price sold ($1000) given its age at three points of the grap, when Age = 1, 3, and 5.
  # No outlier
    plot(WePhone$PriceSold[1:25] ~ WePhone$Age[1:25], ylim=c(0, 1100))
    abline(glm(PriceSold~Age,data=WePhone[1:25,]))
  # Outlier at Age = 1
    WePhone[26,]<-c(1,1000)
    plot(WePhone$PriceSold ~ WePhone$Age, ylim=c(0, 1100))
    abline(glm(PriceSold~Age,data=WePhone))
  # Outlier at Age = 3
    WePhone[26,]<-c(3,1000)
    plot(WePhone$PriceSold ~ WePhone$Age, ylim=c(0, 1100))
    abline(glm(PriceSold~Age,data=WePhone))
  # Outlier at Age = 5
    WePhone[26,]<-c(5,1000)
    plot(WePhone$PriceSold ~ WePhone$Age, ylim=c(0, 1100))
    abline(glm(PriceSold~Age,data=WePhone))

# These graphs reveal the answer to the question about when an outlier in a regression model's dependent variable, "Y", causes the least amount of bias (or change in slope) in its relationship to an independent variable, "X". Answer the solution with "A", "B", or "C". Answer options: A) When the outlier in Y occurs at low values of X, B) When the outlier in Y occurs at middle values of X, C) When the outlier in Y occurs at high values of X."

#---- Question 1-------------------------------------#
      Solution1<-""
#----------------------------------------------------#

```
*** =solution
```{r}
    Solution1<-"B"

```
*** =sct
```{r}
test_object("Solution1")

success_msg("Good work! Outliers typically effect the slope of a bivariate regression line least when they occur at middle values of X. When an outlier significantly alters the slope of a regression line, we refer to it as an 'influence point.' These typically need to be dealt with or removed to produce reliable regression results (i.e. a single outlier could mask the entire relationship between two variables). As a caveat, it should be noted that outliers do not always bias bivariate relationships (e.g. when an unusally high value of Y occurs with an unusually high value of X), but such outliers tend to have great statistical 'leverage,' which can lead to substantial bias in multivariate relationships (such as when a regression model includes more than one key independent variable). ")
```



--- type:NormalExercise lang:r xp:50 skills:1
## Toying with OLS II: Statistical Power
Since OLS models are so easy to visualize, they provide a nice tool for understanding why models that use more observations tend to have more statistical power. In the following question, we have two datasets provided by the popular online auctioneer eGulf about the relationship between WePhone age and sales price, one with 25 observations. For each of these datasets, we add 25 randomly generated observations that have no relationship between WePhone age and sales price, to represent "statistical noise" - unexplained variation in our sample. Follow the sample code to create regression models with these two datasets, and answer the following question:

*** =instructions
- Which regression model most accurately represents the true relationship between WePhone `Age` and `SalesPrice`?
*** =pre_exercise_code
```{r}
#Original dataset
set.seed(1)
library(ggplot2)
WePhone<-data.frame(age=rep(c(1,2,3,4,5),5))
WePhone$Feedback<-round(rnorm(n=25,mean=90,sd=3))
WePhone$Age<-WePhone$age+rnorm(n=25,mean=0,sd=.3)-WePhone$Feedback/10+mean(WePhone$Feedback/10)
WePhone$Value<-500-(9*WePhone$age^2)+ round(rnorm(n=25,mean=0,sd=30))
names(WePhone)[4]<-"Price Sold"
WePhone$`Price Sold`<-WePhone$`Price Sold`+20*(WePhone$Feedback-mean(WePhone$Feedback))
WePhone$age<-NULL
WePhone$Feedback<-NULL
names(WePhone)[2]<-"PriceSold"
WePhone1<-WePhone

#Second dataset
WePhone<-data.frame(age=rep(c(1,2,3,4,5),20))
WePhone$Feedback<-round(rnorm(n=100,mean=90,sd=3))
WePhone$Age<-WePhone$age+rnorm(n=100,mean=0,sd=.3)-WePhone$Feedback/10+mean(WePhone$Feedback/10)
WePhone$Value<-500-(9*WePhone$age^2)+ round(rnorm(n=100,mean=0,sd=30))
names(WePhone)[4]<-"Price Sold"
WePhone$`Price Sold`<-WePhone$`Price Sold`+20*(WePhone$Feedback-mean(WePhone$Feedback))
WePhone$age<-NULL
WePhone$Feedback<-NULL
names(WePhone)[2]<-"PriceSold"
WePhone2<-WePhone

#Random noise
noise<-data.frame(Age=c(1,2,3,4,5)+rnorm(n=25,mean=0,sd=.3))
noise$PriceSold<-round(rnorm(n=25,mean=400,sd=100))

#Append random noise
WePhone1<-rbind(WePhone1,noise)
WePhone2<-rbind(WePhone2,noise)
rm(list = c("noise","WePhone"))
```

*** =sample_code
```{r}
# We have two datasets, WePhone1 and WePhone2. The first contains 25 real observations and 25 random observations; the second contains 100 real observations and 25 random observations. As a reminder, each dataset contains two variables, "Age"" and "PriceSold." 
    summary(WePhone1)
    summary(WePhone2)

# Build regression models using the glm function, that shows the effect of Age on PriceSold for WePhones in each dataset.

#---- Question 1 - glm in dataset WePhone1-----------#
      Solution1<-glm()
#----------------------------------------------------#


#---- Question 2 - glm in dataset WePhone2-----------#
      Solution2<-glm()
#----------------------------------------------------#

# Which of these models better represents the relationship between Age and PriceSold? Solution1 or Solution2?
      
#---- Question 3 - glm in dataset WePhone2-----------#
      Solution3<-""
#----------------------------------------------------#

```
*** =solution
```{r}
    Solution3<-"Solution2"
```
*** =sct
```{r}
test_object("Solution3")

success_msg("Good work! Larger datasets tend to be more robust to 'statistical noise' - unexplained variation in the sample. In this example, statistical noise decreased the relationship between Age and PriceSold (i.e. the coefficient of Age in the model). However, it is possible for statistical noise to induce spurious relationships as well.")
```



--- type:VideoExercise lang:r aspect_ratio:62.5 xp:50 skills:1 key:93014adc4b
## Common Statistical Terms and Transformations in Regression Models
*** =video_link
//player.vimeo.com/video/217554577



--- type:NormalExercise lang:r aspect_ratio:62.5 xp:50 skills:1 key:43c6c78dff
## Practice Creating a Regression Model With Interaction Effects
An experiment conducted by the transportation network company, Unter Technologies, tried to determine whether downsizing their Human Resources (HR) department would lead to higher employee turnover. Using t.tests, they found that the effect of downsizing their HR department would have different conditional average treatment effects (CATE) on men and women. This was determined by separately studying the average treatment effect (ATE) of downsizing on a sample of men and on a sample of just women. However, slicing data this way can substantially reduce one's statistical power, and becomes unwieldy when a data scientist wants to determine more than a couple of CATEs with a given sample. For example, if race is also and important factor in whether employees plan to leave Unter if Unter reduces their HR department, we would need to slice the data several more times, and run several distinct t.tests on the data.

A CATE is basically an example of **statistical moderation** (also known as an interaction effect), where the effect of an independent variable is moderated by the effect of a second independent variable. A good example of moderation on a  causal effect could be seen in your sink faucets. When your water valves are shut, water does not come out, but when you open your valves, water comes out. The valves are not the direct cause of water come out of your pipes - pressure is - but the valves **moderate** the relationship between the pressure in your pipes and your sink. 

In other words, statistical moderation occurs when the size of one independent variable's effect on an outcome is effected by a second independent variable. In this example, we would say that gender moderates the effect of the treatment (downsizing HR) on intention to leave Unter Technology. With the dataframe, `UnterHR`, construct three regression models: One that naively estimates the average treatment effect of reducing the size of Unter's HR department on employee turnover, a second that includes a statistical interaction (to allow for moderation) between treatment and gender (`Female`), and a third that includes interactions between treatment and gender (`Female`) and treatment and race (`Race`).

*** =instructions
- Construct a regression model that measures the effect of `Treatment` on `LeaveJob`, with a mediation effect for `Female`
- Construct a regression model that measures the effect of `Treatment` on `LeaveJob`, with a moderation effect for `Female`
- Construct a regression model that measures the effect of `Treatment` on `LeaveJob`, with separate interactions with `Female` and `Race`
*** =pre_exercise_code
```{r}
set.seed(1)
n=682
#Create Dataframe
  UnterHR<-data.frame(Treatment=rbinom(n,1,.4),Female=rbinom(n,1,.1),LeaveJob=0,Race=sample(4,n,prob=c(.6,.1,.1,.2),replace=T))
#Rename race
  UnterHR$Race<-as.factor(ifelse(UnterHR$Race==1,"White",ifelse(UnterHR$Race==2,"Black",ifelse(UnterHR$Race==3,"Latino","Asian"))))
#LeaveJob
  #treatment makes men less likely to leave
    UnterHR$LeaveJob[UnterHR$Treatment==0 & UnterHR$Female==0]<-rbinom(length(UnterHR$LeaveJob[UnterHR$Treatment==0 & UnterHR$Female==0]),1,.3)
    UnterHR$LeaveJob[UnterHR$Treatment==1 & UnterHR$Female==0]<-rbinom(length(UnterHR$LeaveJob[UnterHR$Treatment==1 & UnterHR$Female==0]),1,.2)
  #treatment makes wommen more likely to leave
    UnterHR$LeaveJob[UnterHR$Treatment==0 & UnterHR$Female==1]<-rbinom(length(UnterHR$LeaveJob[UnterHR$Treatment==0 & UnterHR$Female==1]),1,.3)
    UnterHR$LeaveJob[UnterHR$Treatment==1 & UnterHR$Female==1]<-rbinom(length(UnterHR$LeaveJob[UnterHR$Treatment==1 & UnterHR$Female==1]),1,.6)
  #treatment makes blacks and latinos more likely to leave (redraw if leave job = 0 and and black or latino = 1)
    #count people in either category
      n2<-length(UnterHR$LeaveJob[UnterHR$Treatment==1 & UnterHR$LeaveJob==0 & (UnterHR$Race=="Black" | UnterHR$Race=="Latino" )])
    #redraw for those
      UnterHR$LeaveJob[UnterHR$Treatment==1 & UnterHR$LeaveJob==0 & (UnterHR$Race=="Black" | UnterHR$Race=="Latino" )]<-rbinom(n2,1,.5)
      
      
```
*** =sample_code
```{r}
# Before running a regression model, let's examine the data
    str(UnterHR)

# The dataset contains four variables: Treatment, Female, LeaveJob, and race. Notice that the structure command refers to race as a "Factor" variable. This is a typical way that R identifies nominal (non-numeric) variables.
    summary(UnterHR$Treatment)
    summary(UnterHR$Female)
    summary(UnterHR$LeaveJob)
    summary(UnterHR$Race)

# Question 1 asks us construct a regression model that measures the effect of Treatment on LeaveJob, with a "mediation effect"" for Female. This is the same as "controlling" for Female, as we did for other independent variables in our previous examples. For reference, the syntax for glm is "glm(Y~X+Z,data=df)" where Y is the name of the independant variable, X is the name of the dependent variable, Z is the name of the mediation variable, and df is the name of the dataframe.

#---- Question 1-------------------------------------#
      Solution1<-glm()
#----------------------------------------------------#
    
# If you entered Solution1 correctly and summarize the results via summary(Solution1), the effect for Treatment should be about .047 (but non significant), and the effect for Female should be about .16. The direct interpretation of this model is that Treatment increases the odds that a person will leave their job by .046 (4.6%), and being Female increases the odds that a person will leave their job by .16, regardless of Treatment. 
      
# The model in Solution1 does not assume that Treatment effects women differently, but that women have a higher baseline chance of leaving their job, regardless of the treatment effect. In order to find out if the treatment effects women differently, we need to add an interaction between Female and Treatment. For reference, the syntax for glm with an interaction effect is "glm(Y~X*Z,data=df)" where Y is the name of the independant variable, X is the name of the dependent variable, Z is the name of the mediation variable, and df is the name of the dataframe. The difference between the syntax for a mediation and moderation effect is that we multiply X and Z for moderation, rather than add them. Construct a regression model that measures the effect of`Treatment on LeaveJob, with a moderation effect for Female.

#---- Question 2-------------------------------------#
      Solution2<-glm()
#----------------------------------------------------#

# By default, R includes independent variables for Treatment and Female when we add a statistical interaction term. The coefficient for Treatment indicates the effect of Treatment for non-women (men); the coefficient for Female indicates the effect of being a female, independent of Treatment. The Treatment:Female coefficient indicates the effect of Treatment for Females. The large coefficient for this interaction effect suggests that Treatment has a large effect on women's odds of leaving their job.
      
# Let's now answer the final question for this exercise. We need to include an interaction effect between Treatment and Female, and an interaction effect between Treatment and Race. For reference, the syntax for glm with two interaction effects is "glm(Y~X*Z1+X*Z2,data=df)" where Y is the name of the independant variable, X is the name of the dependent variable, Z1 is the name of the first mediation variable, Z2 is the name of the first mediation variable and df is the name of the dataframe.
      
#---- Question 3-------------------------------------#
      Solution3<-glm()
#----------------------------------------------------#
      
      
      
```
*** =solution
```{r}
      Solution1<-glm(LeaveJob ~ Treatment + Female,data=UnterHR)
      Solution2<-glm(LeaveJob ~ Treatment * Female,data=UnterHR)
      Solution3<-glm(LeaveJob ~ Treatment * Female + Treatment * Race,data=UnterHR)
```
*** =sct
```{r}
#test_object("Solution1")
#test_object("Solution2")
#test_object("Solution3")
success_msg("Good work! It appears that the treatment only affected individuals who were Female, Black, or Latino.")
```



--- type:VideoExercise lang:r aspect_ratio:62.5 xp:50 skills:1 key:7fb39b7cb6
## Logistic Regression Models
*** =video_link
//player.vimeo.com/video/217554577



--- type:VideoExercise lang:r aspect_ratio:62.5 xp:50 skills:1 
## Defining The Average Effect of Treatment on the Treated
*** =video_link
//player.vimeo.com/video/217554577


--- type:MultipleChoiceExercise lang:r xp:50 skills:1 
## Average Effect of Treatment on the Treated
A scientist develops a new drug to help reduce addiction to pain killers. He administers this drug to many recent abusers of painkillers, and finds that these patients had substantially lower rates of painkiller abuse than would be expected, given that painkiller addicts often relapse following rehab. However, across the **entire human population**, the scientist expects the average treatment effect of his drug to be negligible on rates of painkiller abuse. Why might the scientist expect the average treatment effect on the treated to be larger than the average treatment effect? 

*** =instructions
- Because those who were treated with the drug were more likely to gain benefit from using it.
- Because painkiller abusers are an unreliable sample. 
- Because the sample who tried the scientist's drug was small.
*** =sct
```{r}
msg1 = "Correct! The only people who were administered the drug were painkiller addicts, but this is a relatively small portion of the population. The drug was designed to reduce addiction to painkillers, so cannot help the majority of individuals in the population who are not addicted to pain killers"
msg2 = "Not quite. Think through why we care about ATT and try again"
msg3 = "Actually, unless we know that a study's sample was biased, we have no reason to expect the observed treatment effect to increase or decrease. Try again."
test_mc(correct = 1, feedback_msgs = c(msg1,msg2,msg3))
```

 

--- type:VideoExercise lang:r aspect_ratio:62.5 xp:50 skills:1 
## How to Compute ATE Under Unconfoundedness, and What Not to Do
*** =video_link
//player.vimeo.com/video/217554781


--- type:NormalExercise lang:r aspect_ratio:62.5 xp:50 skills:1
## Practice with Survey Weights
Given the diversity of people in the United States, many surveys have difficulty gathering perfectly representative samples of the population. This can be problematic when your research results are meant to be generalizable to the population at large. However, surveyors typically know the true proportion of demographic traits among the U.S. population, and provide survey weights that compensates for their sampling bias, by increasing the importance of underrepresented groups in individual's statistical analyses. In this problem, we will practice using survey weights.

The American film studio, Delimited Pictures, is interested in how many tickets they should expect to sell to adults for their upcoming film, Cambrian Park, and what age group are most likely to watch it. They interview people across the country, asking them whether they plan to see the movie when it comes out. Although their resulting sample was not perfectly representative of the country's adult population, a statistician was able to provide them with survey weights that compensate for this sampling error. Use these survey weights and the dataset `Survey` to help Delimited Pictures estimate what proportion of the U.S. adult population will see their upcoming movie, and to determine what age group is most likely to watch it.


*** =instructions
- Estimate the proportion of the US adult population that plans to watch Cambrian Park (variable `WillWatch`) while using the provided survey weights.
- Construct a survey-weighted OLS regression model to estimate the effect of `Age` on `WillWatch` controlling for `Female`.

*** =hint
- This is a useful hint
*** =pre_exercise_code
```{r}
library(survey)
set.seed(1)
n=1775
#Create Dataframe
  Survey<-data.frame(Black=rbinom(n,1,.1))
  Survey$Latino<-ifelse(Survey$Black==1,0,rbinom(n-sum(Survey$Black),1,.07))
  Survey$White<-ifelse(Survey$Black==1 | Survey$Latino==1,0,rbinom(n-sum(Survey$Black)-sum(Survey$Latino),1,.95))
  Survey$Other<-ifelse(Survey$Black==1 | Survey$Latino==1 | Survey$White==1,0,1)
  Survey$Female<-rbinom(n,1,.6)
  Survey$Age<-round(c(rnorm(.7*n+1,mean=33,sd=4),rnorm(.1*n,mean=45,sd=5),rnorm(.2*n,mean=55,sd=5)))
  Survey$Weight<-1.35*Survey$Black+1*Survey$White+1.3*Survey$Latino+1.07*Survey$Other+Survey$Age*.005+.3*(1-Survey$Female)-.3699
  Survey$WillWatch<-rbinom(n,1,.01+.03*(1-Survey$Female)+.01*Survey$Black+.00003*Survey$Age^2)
```
*** =sample_code
```{r}
# To get used to what survey weights like look like, let's summarize the Weight variable in our dataset. The mean weight ranges from around 0.5 to 2.0 in this dataset, but other survey datasets may include weights with much larger numbers. In general, individuals with higher weights were undersampled. Having a large weights gives an observation more influence in your statistical models. 
    summary(Survey$Weight)
    
# As a teaser to what we are doing when we "weight" the data, let's look at the variable for gender, 'Female`. The first line below shows the proportion of females in this survey. This number is much too uneven to be representative of the actual US population. The second line indicates the proportion of females in these survey when they are weighted. These numbers appear much more realistic.   
    prop.table(table(Survey$Female))
    prop.table(xtabs(Weight~Female, data=Survey))

# What proportion of the U.S. population is planning to watch Cambrian Park? Without survey weights, we get the following results:
    mean(Survey$WillWatch)
    
# This is no good. Limited Pictures expected that at least 7% of the population would watch their film. What about if we use the survey weights to get a more representative estimation of the proportion of people who will watch the film? Try to compute the waited mean for WillWatch. The syntax for determining the weighted mean of a variable is the following: weighted.mean([VariableOfInterest],[WeightVariable]). 

#---- Question 1-------------------------------------#
    Solution1<-weighted.mean()
#----------------------------------------------------#
    
# It looks like the original sample underrepresented the proportion of people who might actually watch Cambrian Park. The weighted mean is above 7%! Let's now construct a survey-weighted OLS regression model that estimates the effect of Age on WillWatch, controlling for Female. The syntax is almost identical to normal regression models. We simply need to add a statement to identify survey weights. The syntax is `glm(Y ~ X + Z, weight = [Weight], data = [dataframe])`, where Y = the name of the dependent variable, X = the name of the independent variable, Z = the name of the control variable Weight = the weight variable, and dataframe = the dataframe that contains these variables.

#---- Question 2-------------------------------------#
    Solution2<-glm()
#----------------------------------------------------#
```
*** =solution
```{r}
Solution1<-weighted.mean(Survey$WillWatch,Survey$Weight)
Solution2<-glm(WillWatch ~ Female+Age, weight=Weight, data=Survey)
```
*** =sct
```{r}
test_object("Solution1")
test_object("Solution2")
success_msg("Good work! Although survey weights do not often change one's results substantially, it is still good practice to use them if they are provided to you, especially if you are trying to study trends about an entire population. If you are doing extensive analyis with weighted data, you may want to consider using the 'survey' package instead of the weight options in R's built in functions.")
```


--- type:MultipleChoiceExercise lang:r xp:50 skills:1 
## When Survey Weights Are Unnecessary 
One week after Cambrian Park was released, Delimited Pictures gathered a survey of people who watched it and asked them whether or not they liked the film. Delimited Pictures put significant effort into catering to a younger female audience - individuals who had the least positive reaction to Cambrian Park's prequel, Neoproterozoic Park. If Delimited Pictures is just interested in whether female college students in Los Angeles seemed to enjoy Cambrian Park, do they need to use survey weights to poststratify their sample?  

*** =instructions
- Yes
- No
*** =sct
```{r}
msg1 = "Try again"
msg2 = "Correct! Survey weights are only useful when you are trying to generalize a finding to the general population. They are typically only used for analysis of national trends. Since Delimited Pictures is only interested in how a narrow demographic perceived their new film, it is unlikely that they would need to weight their data."
test_mc(correct = 2, feedback_msgs = c(msg1,msg2))
```

