--- 
title       : "Regressions 1: Introduction to Regression As Causality"
description : "This chapter will introduce you to using regression analysis to find causal effects"
 
 
  
--- type:VideoExercise lang:r aspect_ratio:62.5 xp:50 skills:1 key:5c9d6dd3fc
## Introduction to Regression Analysis
*** =video_link
//player.vimeo.com/video/217553687



--- type:MultipleChoiceExercise lang:r xp:50 skills:1 key:1a2137dd19
## Interpreting Regressions
The popular online auctioneer, eGulf, wants to create an algorithm that will automatically provide recommended sales prices to eGulf sellers. In the case of wePhones, eGulf plans to base its algorithm on the age and sales price of previously sold WePhones. For the sake of convenience, eGulf bases their algorithm on a small stratified sample of 25 WePhones sold in the past week. 

eGulf runs an OLS regression to determine the relationship between WePhone age and sales price, and plots their results (illustrated in the R workspace). Each point on the plot indicates the age and selling price for a particular WePhone, and the line shows the results from an OLS regression on this data. What do these results suggest about the relationship of WePhone age and sales price?


*** =instructions
- There is a negative relationship
- There is no relationship
- There is a positive relationship
*** =pre_exercise_code
```{r}
set.seed(1)
library(ggplot2)
WePhone<-data.frame(age=rep(c(1,2,3,4,5),5))
WePhone$Feedback<-round(rnorm(n=25,mean=90,sd=3))
WePhone$Age<-WePhone$age+rnorm(n=25,mean=0,sd=.3)-WePhone$Feedback/10+mean(WePhone$Feedback/10)
WePhone$Value<-500-(9*WePhone$age^2)+ round(rnorm(n=25,mean=0,sd=30))
names(WePhone)[4]<-"Price Sold"
WePhone$`Price Sold`<-WePhone$`Price Sold`+20*(WePhone$Feedback-mean(WePhone$Feedback))
model<-lm(`Price Sold`~Age,data=WePhone)
ggplot(data=WePhone,aes(Age, `Price Sold`))+geom_point()+geom_abline(intercept = model$coefficients[1],slope=model$coefficients[2])+ ggtitle("Scatter Plot and OLS Regression of WePhone Age on Price Sold")
```
*** =sct
```{r}
msg1 = "Correct! As the age of WePhones increased, the price at which they were sold decreased."
msg2 = "Although the plot does not indicate if their results were statistically significant, they do indicate a relationship. Try again"
msg3 = "Think this through carefully. Does an increase in age increase or decrease the price sold of a WePhone? Try again"
test_mc(correct = 1, feedback_msgs = c(msg1,msg2,msg3))
```



--- type:MultipleChoiceExercise lang:r xp:50 skills:1 key:4b9a40fa01
## Reversing Causal Direction
Causal interpretation of regression models can be tricky. Using the same data in the last example, an eGulf analyst decided to toy with a regression model where age is a function of price sold (i.e. price is the independent variable, and age is the dependent variable). The results of this model are visualized in the R workspace. If this model shows a causal relationship between age and price sold, with age as the dependent variable, what **exactly** should the analyst say to describe the results to his boss? 

*** =instructions
- Older phones tend to sell for less
- Old age causes phones to sell for less
- High sales prices causes phones to become younger
*** =pre_exercise_code
```{r}
set.seed(1)
library(ggplot2)
WePhone<-data.frame(age=rep(c(1,2,3,4,5),5))
WePhone$Feedback<-round(rnorm(n=25,mean=90,sd=3))
WePhone$Age<-WePhone$age+rnorm(n=25,mean=0,sd=.3)-WePhone$Feedback/10+mean(WePhone$Feedback/10)
WePhone$Value<-500-(9*WePhone$age^2)+ round(rnorm(n=25,mean=0,sd=30))
names(WePhone)[4]<-"Price Sold"
WePhone$`Price Sold`<-WePhone$`Price Sold`+20*(WePhone$Feedback-mean(WePhone$Feedback))
model<-lm(Age~`Price Sold`,data=WePhone)
ggplot(data=WePhone,aes(`Price Sold`,Age))+geom_point()+geom_abline(intercept = model$coefficients[1],slope=model$coefficients[2])+ ggtitle("Scatter Plot and OLS Regression of WePhone Age on Price Sold")
```
*** =sct
```{r}
msg1 = "This phrasing does not address a causal interpretation of the relationship, but rather is a description of an association between WePhone age and sales price"
msg2 = "This is the most intuitive explanation for the relationship between WePhone sale price and age, but not for this particular regression model, which was designed to use age as the dependent variable. Try again"
msg3 = "Correct! Well, at least according to this model. This is an example of reverse causality. Obviously, we know that in reality the age of any object can only be influenced by time, so we know that this interpretation of their causal relationship is false: price sold cannot cause a phone's age. The analyst should try a different model next time."
test_mc(correct = 3, feedback_msgs = c(msg1,msg2,msg3))
```




--- type:MultipleChoiceExercise lang:r xp:50 skills:1 key:d50636b4ac
## Regression Models and Policy Implementation
Suppose you see a Facebook post with a regression plot of the Number of Walmarts per Capita on Income per Capita, and the slope is positive. If you were asked to advise a town on a policy to attract a Walmart to town, would you say they should give tax refunds to their citizens to increase their income?
*** =instructions
- Yes, the regression shows that Walmarts come to richer places
- No, you can't learn the cause of Walmart locations from the regression
*** =sct
```{r}
msg1 = "Whoops. Try again."
msg2 = "No, you can't learn the direct causal effect from this regression as described, because there are so many other social and economic factors to consider in this relationship that Unconfoundness Assumption is unlikely to be true. In this case, we can just learn the amount of correlation between the variables."
test_mc(correct = 2, feedback_msgs = c(msg1,msg2))
```


--- type:VideoExercise lang:r aspect_ratio:62.5 xp:50 skills:1 key:8bb34ee718
## Basic Elements of a Regression Table
*** =video_link
//player.vimeo.com/video/217554002



--- type:MultipleChoiceExercise lang:r xp:50 skills:1 key:470a340b81
## Reading a Bivariate Regression Table
The following table shows the regression results from the previous questions about eGulf's pricing algorithm for used wePhones. The coefficient for age reflects the slope in the previous line graph, and it suggests that for each additional year (or unit of age), the expected sales price of WePhones decreases by $61.99. The coefficient for the intercept matches the Y-axis intercept in the previous line graph. It indicates the predicted price sold for a WePhone at 0 years old (or more generally, it shows the value of an outcome when all other independent variables are at their reference point). 

| Variable   | Coefficient (SE) |
|------------|-----------------:|
|    Age     |   -61.99 (7.28)  |
| Intercept  |   592.56 (24.42) |

To determine the predicted sales price of a WePhone with a given age, we can start with the initial intercept value and add to it the total change in price for the age of the phone. In other words, based on this regression model the formula for predicting the price of a WePhone sold on eGulf is:

`Sales Price = 592.56 + Age * -61.99`

Based on this formula, what is a valid predicted price for a WePhone that is exactly two years old?



*** =instructions
- 460.49
- 468.58
- 472.57 
- 475.56
*** =sct
```{r}
msg1 = "Try again"
msg2 = "Well done! Notice any advantages of regression models versus t.tests? In a t.test, we can only predict the outcome from a binary treatment variable (that is, the outcome for respondents in the treatment versus control group), In a regression model, we can predict the outcome from a continuous treatment variable (e.g. age)."
msg3 = "Try again"
msg4 = "Try again"
test_mc(correct = 3, feedback_msgs = c(msg1,msg2,msg3,msg4))
```



--- type:MultipleChoiceExercise lang:r xp:50 skills:1 key:9917eaebf2
## Reading a Multivariate (Multiple) Regression Table
In our previous eGulf example, we assumed that age is the only determinant of a WePhone's sales price. However, sellers on eGulf also have "feedback scores," ranging from 0-100, that indicate the quality of their previous transactions with customers. It seems likely that a seller's feedback score could positively impact a WePhone's sales price; that is, customers might be willing to pay more for WePhones sold by reputable sellers. However, if reputable sellers tended to sell newer WePhones, this would confound the relationship between sales price and age. 

We could consider overcoming this by throwing out respondents with high feedback scores that sold new WePhones, so that WePhone age and WePhone feedback score were no longer correlated. That might let us look at the effect of the feedback score in a natural experiment analysis, but throwing out those high scores might imbalance our sample and make that analysis invalid. However, with OLS regression, we can simply add seller feedback scores as another determinant of our independent variable, thereby controlling for its confounding effect on our outcome variable. This is what is shown in the table below.

| Variable   | Coefficient (SE) |
|------------|-----------------:|
|    Age     |  -55.61 (4.94)   |
|  Feedback  |   14.15 (2.55)   |
| Intercept  | -707.23 (234.38) |


As you can see, when controlling for seller score, age has an even larger negative effect on sales. Using this table, calculate the difference (in absolute terms) in the expected sales price for phone that is 2 year old with a feedback score of 80, versus a phone that is 4 years old with a feedback score of 90.


*** =instructions
- 22.85
- 26.83 
- 27.00
- 30.28
*** =sct
```{r}
msg1 = "Try again"
msg2 = "Try again"
msg3 = "Try again"
msg4 = "Well done! The same method can applied with as many variables as we choose. This provides a great way of examining the trade offs between different factors that effect on outcome. However, notice that the intercept is -707.23. That would indicate that a used WePhone with an age of 0 and a feedback score of 0 would have a predicted sales price of -$707.23. Obviously that is unrealistic; no sales on eGulf result in sellers having to pay their customers. This hints at a modeling issue that we'll explore later in the chapter."
test_mc(correct = 4, feedback_msgs = c(msg1,msg2,msg3,msg4))
```



--- type:VideoExercise lang:r aspect_ratio:62.5 xp:50 skills:1 key:ffa5133d86
## The Relationship between Economic Development and Property Rights
*** =video_link
//player.vimeo.com/video/217554163




--- type:MultipleChoiceExercise lang:r xp:50 skills:1 key:fd77ad9990
## Regressions with small coefficients and small confidence intervals
Does a regression coefficient of 0 with a small confidence interval / small standard errors imply that the outcome and treatment variables simultaneously cause each other?
*** =instructions
- Yes, it shows the variables cancel each other out.
- No, it just shows there's no relationship between these variables in the data.
- It shows "misspecification" and we can't learn anything
*** =sct
```{r}
msg1 = "Actually, OLS regression models can't show simultaneous causality. More advanced methods are needed for this. Try again."
msg2 = "Regressions by themselves are about describing relationships. So a zero regression coefficient shows that there is no relationship in the data between these variables. It does *not* say anything about the causal effect of one variable on the other, unless we make additional assumption, like unconfoundedness. Without extra assumptions, it is possible to have simultaneous causality and yet find a regression coefficient of zero."
msg3 = "Although misspecification might cause a regression model to have no coefficients for variables that should be associated with the outcome of interest, this is not necessarily the case. Moreover, every regression model can teach you something; at the very least, a misspecified regression models can teach us how *not* to specify a regression model for our outcomes of interest."
test_mc(correct = 2, feedback_msgs = c(msg1,msg2,msg3))
```


--- type:MultipleChoiceExercise lang:r xp:50 skills:1 key:f47fad8c5f
## Recalling terminology
Why do we use the words "left hand side" variable and "right hand side" variable in casual inference?

*** =instructions
- It refers to how you graph them on X and Y axis.
- It comes from historical, more qualitative writings.
- It refers to the left and right hand side of an equation.
*** =sct
```{r}
msg1 = "Good try, but remember that on a graph, the Y axis is usually marked on the left side of the page, and the X axis is usually marked on the *bottom* of the page. Try again."
msg2 = "Statistics is a relatively young and quantitative (almost by definition) field. Try again."
msg3 = "When we write down a regression mathematically as an equation, we typically write it as 'Y=beta*X + U' where Y is our outcome variable and X is our treatment or policy variable. Beta here is the regression coefficient and U represents unobserved variables besides X that also affect Y. So 'left hand side' just refers to the left hand side of the equal sign in this equation, while 'right hand side' refers to the right hand side of the equal sign in this equation."
test_mc(correct = 3, feedback_msgs = c(msg1,msg2,msg3))
```


--- type:MultipleChoiceExercise lang:r xp:50 skills:1 key:408e61980c
## Multiple Regression Models
Papers often report several regression models next to each other. This table shows a few truncated models from the example in the video, about the relationship between a country's property rights protections and its GDP. Model 1 shows the coefficient and standard error for the effect of property rights protection on GDP without any control variables. Model 2 shows the effect of property rights protection on GDP while controlling for the country's latitude, and Model 3 shows the same, while also controlling for whether the country is in Asia. Why might the effect of property rights protections on GDP decrease as we add more variables to our model?   

| Variable   | Model 1 | Model 2 | Model 3 |
|------------|--------:|--------:|---------|
| Protection |.52(.06) |.47(.06) |.43(.05) |
| Latitude   |         |.89(.49) |.37(.51) |
| Asia Dummy |         |         |-.62(.19)|


*** =instructions
- Because the inclusion of more variables weakens the statistical power of an independent variable
- Because omitting these variables caused the effect of property rights protection on GDP to be downwardly biased. 
- Because the inclusion of more variables strengthens the statistical power of an independent variable
- Because omitting these variables caused the effect of property rights protection on GDP to be upwardly biased. 
*** =sct
```{r}
msg1 = "Although this is often true, this is not indicated in this figure. If you look at the standard errors, the statistical significance of property rights protections on GDP is relatively small and consistent across models. Try again."
msg2 = "This would mean that the omitted variables confound the relationship between property rights protections and GDP by decreasing the it. Try again"
msg3 = "This is usually not true, nor shown in this figure. Try again."
msg4 = "Correct! Without controlling for these variables, the relationship between property rights protections and GDP seemed larger than it truly was."
test_mc(correct = 4, feedback_msgs = c(msg1,msg2,msg3,msg4))
```



--- type:NormalExercise lang:r aspect_ratio:62.5 xp:50 skills:1 key:631cb11d99
## Practice Running a Regression Model
Dr. Max Funn is interested in understanding the effect of income on happiness. He has a small sample of the General Social Survey (GSS), which asked respondents about their income and happiness independently. Although Dr. Funn predicts that income is positively associated with happiness, he is worried that the relationship between income and happiness might be confounded by their joint relationship with work hours. Specifically, Dr. Funn believes that people who work more hours make more money (which indirectly increases a person's happiness), but that people who work more hours at the same income tend to be less happy (i.e. work hours has a direct negative effect on happiness). 

Using the dataset, `GSS`, help Dr. Max Funn measure the effect of income on happiness, **controlling** for work hours. Specifically:


*** =instructions
- Construct a regression model that measures the effect of income on happiness.
- Construct a second regression model that measures the effect of income on happiness, controlling for work hours.

*** =pre_exercise_code
```{r}
n=1000
set.seed(1)
#Create rnorm function that allows for min and max
  rtnorm <- function(n, mean, sd, min = -Inf, max = Inf){
      qnorm(runif(n, pnorm(min, mean, sd), pnorm(max, mean, sd)), mean, sd)
  }
#Create rounding function that allows to round to numbers above 1
  mround <- function(x,base){ 
          base*round(x/base) 
  } 
#Create scaling function that puts number between 0 and 1
  scale <- function(x){
    (x - min(x))/(max(x)-min(x))
    }

#Dataframe
  GSS<-data.frame(WorkHours=1:n,Income=1:n,Happiness=1:n)
#Work hours 
  GSS$WorkHours<-round(rtnorm(n=n,mean=40,sd=6,min=0,max=55))
#Income (in thousands)
  GSS$Income<-GSS$WorkHours*.5+mround(rtnorm(n=n,mean=30,sd=5,min=0,max=100),.1)
#Scaled variables
  scaledincome<-scale(GSS$Income)
  scaledWorkHours<-scale(GSS$WorkHours)
#Happiness (set as 1:5; rounded to 1; make income 2* as effective as work hours)
  GSS$Happiness<-round(scaledincome-(.5*scaledWorkHours) + rtnorm(n=n,mean=3,sd=.5,min=0,max=5),0)
  
  summary(GSS$WorkHours)
  summary(GSS$Income)
  summary(GSS$Happiness)
    cor(GSS$WorkHours,GSS$Income)
    
```
*** =sample_code
```{r}
# Before running a regression model, let's examine the data
    str(GSS)

# The dataset contains three variables: income (measured in thousands of dollars), work hours, and happiness (measured 1=very unhappy, 2=unhappy, 3=neither happy or unhappy, 4=happy, 5=very happy)
    summary(GSS$Income)
    summary(GSS$WorkHours)
    summary(GSS$Happiness)

# Using the correlation function, we can tell that income and happiness are positively correlated. But by how much? What is the effect of increasing one's income by $1000 on their happiness?
    cor(GSS$Income,GSS$Happiness)
  
# Let's use the glm function to do a linear regress of the effect of income on happiness. For reference, the syntax for glm is "glm(Y~X,data=df)" where Y is the name of the independent variable, X is the name of the dependent variable, and df is the name of the dataframe.

#---- Question 1-------------------------------------#
      Solution1<-glm()
#----------------------------------------------------#

# The glm function in R usually provides less information about the model than desired, and usually in a less than ideal format. Let's use the summary function on our model to see its effects more clearly.
    summary(Solution1)

# The table beneath "Coefficients" is what is usually reported in scientific journals. "Estimate" refers to the coefficients for each parameter, "Pr(>|t|)" refers to the p.value of the estimates, and the stars following p.value refer to the result's statistical significance. 
    
# If you entered Solution1 correctly, the effect of Income should be about .015, meaning that a single unit increase in income ($1000) is associated with a .015 increase in a person's reported happiness. The low p.value and stars to the right of the coefficients suggest that this effect is statistically significant. 

# This relationship can also be viewed graphically via the syntax below. The points on the plot represent each observation, and the diagonal line represents the regression results. Note that the slope of the line is equal to the coefficient for income.
    plot(GSS$Happiness ~ GSS$Income)
    abline(Solution1)

# Now let's move on to Question 2.Let's look at the correlations of work hours on happiness and income. They show that work hours are negatively correlated with happiness and positively correlated with income. This suggests that work hours might confound the relationship between income and happiness.
    cor(GSS$WorkHours,GSS$Happiness)
    cor(GSS$WorkHours,GSS$Income)

# The next best step is to control for the effect of work hours on happiness. You can do this for yourself easily in R by adding the control variable directly to the glm function to regress the effect of income on happiness. For reference, the syntax for glm with controls is "glm(Y~X+Z,data=df)" where Y is the name of the independent variable, X is the name of the dependent variable, Z is the name of the control variable, and df is the dataframe.

#---- Question 2-------------------------------------#
      Solution2<-glm()
#----------------------------------------------------#
  
```
*** =solution
```{r}
    Solution1<-glm(Happiness~Income,data=GSS)
    Solution2<-glm(Happiness~Income+WorkHours,data=GSS)
```
*** =sct
```{r}
test_object("Solution1")
test_object("Solution2")
success_msg("Good work! You may have noticed that the coefficient for income increased when we controlled for WorkHours. This is typical when a control variable is positively correlated with both the independent and dependent variables. This syntax can be used to control for as many variables as desired in a regression model.")
```



--- type:VideoExercise lang:r aspect_ratio:62.5 xp:50 skills:1 key:d75a4ac636
## Using Regression to Get Causal Effects: Unconfoundedness
*** =video_link
//player.vimeo.com/video/217554416



--- type:MultipleChoiceExercise lang:r xp:50 skills:1 key:36681d6cec
## Regressing Soda pop and K-pop
You are studying the effect of pop songs on teenage product consumption using a dataset from South Korea. Via a regression model, you find that for every four songs downloaded, soft drink sales tend to increase by about one can. Does this imply that South Korean Soda Companies should start sponsoring pop stars?
*** =instructions
- No
- Yes
*** =sct
```{r}
msg1 = "Correct. The regression model does not necessarily recover a true causal effect of song downloads on soft drink sales."
msg2 = "Whoops. Try again."
test_mc(correct = 1, feedback_msgs = c(msg1,msg2))
```



--- type:MultipleChoiceExercise lang:r xp:50 skills:1 key:5c60df0d16
## Knowing it all
If you could observe every single variable that mattered for determining outcomes, would you be able to learn all the causal effects you want?
*** =instructions
- No, regressions only give you correlation, not causation.
- Yes, you'd have every combination of variables possibly needed for determining causality.
- Yes, but just for that dataset.
- No, it would give you conflicting information.
*** =sct
```{r}
msg1 = "Although it may seem like that is what this course is implying, there are times when regressions can demonstrate a causal relationship. Try again."
msg2 = "According to our definition of causality, if you truly observed every relevant variable, you would indeed be able to learn all the causal effects you want. There are a few caveats though. First, you would still need to ensure that there was at least some variation in each variable you cared about. Even if you observed every variable that mattered, if you never see those variables change across units, you will not be able to learn their causal effects. Second, no real world dataset in social science will truly have all variables you want, which is why the unconfoundedness assumption is often made-to assume that the variables we do not observe are not related to the treatment."
msg3 = "This answer is tricky. Remember, the dataset contains every relevent variable in the universe for determining outcomes. That should make findings from it generalizable beyond the dataset. Try again."
msg4 = "It's true that you might have a hard time finding statistically significant results if your sample was small, as there theoretically could be an infinite number of relevant variables for any outcome in the universe, many of which are correlated. However, many events do have a variety of causes, and many regression models do incorporate a lot of 'conflicting' variables. Try again."
```






--- type:MultipleChoiceExercise lang:r xp:50 skills:1 key:ff6e381314
## The Uncounfoundedness Assumption
Although regression models are great for summarizing the association between variables, any causal interpretation of a regression models rests on the uncounfoundedness assumption (also known as the selection-on-observables assumption; and sometimes referred to as the conditional independence assumption). Which of the following summarizes this assumption?

*** =instructions
- All variables that affect treatment assignment and the outcome of interest are observable and conditioned upon.
- If something happens more frequently than expected during a given period, we can expect that it will happen less frequently in the future
- There is little or no multicollinearity in the data
- The residuals of a regression model are normally distributed.
*** =sct
```{r}
msg1 = "Correct. Put even more simply, any causal interpretation of a regression model assumes that the relationships observed in the data are unconfounded. In most cases, this is impossible to verify empirically. However, it is a pretty large assumption. Most variations of regression models that we will describe in later chapters try to reduce the strength of this assumption."
msg2 = "This is actually a summary of the gambler's fallacy. Try again"
msg3 = "This is an assumption that regression models make, but it is not a description of the unconfoundedness assumption"
msg4 = "This is an assumption that regression models make, but it is not a description of the unconfoundedness assumption"
test_mc(correct = 1, feedback_msgs = c(msg1,msg2,msg3,msg4))
```



--- type:VideoExercise lang:r aspect_ratio:62.5 xp:50 skills:1 key:9b0c9370cf
## How to Compute Regressions: Ordinary Least Squares (OLS)
*** =video_link
//player.vimeo.com/video/217554577


--- type:NormalExercise lang:r xp:50 skills:1 key:1ff9f6d33f
## Toying with the Math Behind Ordinary Least Squares (OLS)
Although the math behind determining an OLS regression model is complicated, understanding what it's trying to do is surprisingly straightforward. Using data provided by the popular online auctioneer eGulf about the relationship between WePhone age and sales price, follow the sample code to determine the sum of squared residuals (SSR) of the regression model.

*** =instructions
- Estimate the sum of squared residuals from the provided regression model.
*** =pre_exercise_code
```{r}
set.seed(1)
library(ggplot2)
WePhone<-data.frame(age=rep(c(1,2,3,4,5),5))
WePhone$Feedback<-round(rnorm(n=25,mean=90,sd=3))
WePhone$Age<-WePhone$age+rnorm(n=25,mean=0,sd=.3)-WePhone$Feedback/10+mean(WePhone$Feedback/10)
WePhone$Value<-500-(9*WePhone$age^2)+ round(rnorm(n=25,mean=0,sd=30))
names(WePhone)[4]<-"Price Sold"
WePhone$`Price Sold`<-WePhone$`Price Sold`+20*(WePhone$Feedback-mean(WePhone$Feedback))
WePhone$age<-NULL
WePhone$Feedback<-NULL
names(WePhone)[2]<-"PriceSold"
```

*** =sample_code
```{r}
# As a quick refresher, let's glimpse at the first few lines of data. Age refers to the number of years old the WePhone was, and Price Sold refers to the dollars it sold for.
    head(WePhone)

# A regression model between a WePhone's Age and Price Sold produces the following table.
    model<-glm(PriceSold~Age,data=WePhone)
    summary(model)
    
# Below visualizes the regression model as a line, and the data points as dots. Notice that the line appears to be placed in between the dots? This is not a coincidence. OLS regression models try to estimate coefficients that create the best fitting line within the data. That is, the line that a regression model produces minimizes the vertical distance (squared) between each data point and the regression line.  
    plot(WePhone$PriceSold ~ WePhone$Age)
    abline(model)
    
# We can actually calculate these distances manually. Let's create a variable that shows the predicted value of each observation based on the regression model. If you remember, the predicted value of any dependent variable can be determined by multiplying each value in the data with the corresponding regression coefficient, and adding them together.
    InterceptCoefficient<-model$coefficients[1]
    AgeCoefficient<-model$coefficients[2]
    WePhone$PredictedPriceSold<-InterceptCoefficient+WePhone$Age*AgeCoefficient

# If we look at the data, we can see that the values for PredictedPriceSold look pretty similar to the value of PriceSold.
    head(WePhone)

# Now for your first task: Create a variable named "residuals" dataset that estimates the PriceSold minus PredictedPriceSold

#---- Question 1-------------------------------------#
      residuals<-
#----------------------------------------------------#

# Now create a variable in the WePhone dataset called residuals2, which is the residuals squared.

#---- Question 2-------------------------------------#
      residuals2<-
#----------------------------------------------------#

# If we sum residuals 2, we get the sum of squared residuals (SSR). Try it now

#---- Question 2-------------------------------------#
      Solution1<-
#----------------------------------------------------#

# This number might seem arbitrary, but it represents the lowest number one can get with this data, given coefficients for Intercept and Age. If we alter the regression coefficients to any number other number, the sume of squared residuals will be larger. For example, if we were to reduce the intercept from the regression model by 5 and increase age by 1 like below, the number we produce with the above algorithm is larger than the number in Solution1. This will be the case no matter how you change the coefficients. This is exactly what ordinary least squares (OLS) regression models optimize: They estimate coefficients for a model that minimizes the squares of differences (or residuals) between the observed responses and those predicted by the model. 
    InterceptCoefficient<-model$coefficients[1]-5
    AgeCoefficient<-model$coefficients[2]+1
    WePhone$PredictedPriceSold<-InterceptCoefficient+WePhone$Age*AgeCoefficient
    sum((WePhone$PriceSold-WePhone$PredictedPriceSold)^2)                
                
```
*** =solution
```{r}
# Prep
    model<-glm(PriceSold~Age,data=WePhone)
    InterceptCoefficient<-model$coefficients[1]
    AgeCoefficient<-model$coefficients[2]
    WePhone$PredictedPriceSold<-InterceptCoefficient+WePhone$Age*AgeCoefficient
#Answer
    residuals<-WePhone$PriceSold-WePhone$PredictedPriceSold
    residuals2<-residuals^2
    Solution1<-sum(residuals2)

```
*** =sct
```{r}
test_object("residuals")
test_object("residuals2")
test_object("Solution1")

success_msg("Good work! Although you will never need to calculate an OLS regression model by hand, you now know precisely what the model is attempting to estimate.")
```





--- type:VideoExercise lang:r aspect_ratio:62.5 xp:50 skills:1 key:59868ce633
## Types of Variables in Regression Models
*** =video_link
//player.vimeo.com/video/217554577



--- type:VideoExercise lang:r aspect_ratio:62.5 xp:50 skills:1 key:93014adc4b
## Common Statistical Terms and Transformations in Regression Models
*** =video_link
//player.vimeo.com/video/217554577



--- type:NormalExercise lang:r aspect_ratio:62.5 xp:50 skills:1 key:43c6c78dff
## Practice Creating a Regression Model With Interaction Effects
An experiment conducted by the transportation network company, Unter Technologies, tried to determine whether downsizing their Human Resources (HR) department would lead to higher employee turnover. Using t.tests, they found that the effect of downsizing their HR department would have different conditional average treatment effects (CATE) on men and women. This was determined by separately studying the average treatment effect (ATE) of downsizing on a sample of men and on a sample of just women. However, slicing data this way can substantially reduce one's statistical power, and becomes unwieldy when a data scientist wants to determine more than a couple of CATEs with a given sample. For example, if race is also and important factor in whether employees plan to leave Unter if Unter reduces their HR department, we would need to slice the data several more times, and run several distinct t.tests on the data.

A CATE is basically an example of statistical moderation (also known as an interaction effect), where the effect of an independent variable is moderated by the effect of a second independent variable. Or in other words, the size of one independent variable's effect on an outcome is effected by a second independent variable. In this example, we would say that gender moderates the effect of the treatment (downsizing HR) on intention to leave Unter Technology. With the dataframe, `UnterHR`, construct three regression models: One that naively estimates the average treatment effect of reducing the size of Unter's HR department on employee turnover, a second that includes an interaction between treatment and gender (`Female`), and a third that includes interactions between treatment and gender (`Female`) and treatment and race (`Race`).

*** =instructions
- Construct a regression model that measures the effect of `Treatment` on `LeaveJob`, with a mediation effect for `Female`
- Construct a regression model that measures the effect of `Treatment` on `LeaveJob`, with a moderation effect for `Female`
- Construct a regression model that measures the effect of `Treatment` on `LeaveJob`, with separate interactions with `Female` and `Race`
*** =pre_exercise_code
```{r}
set.seed(1)
n=682
#Create Dataframe
  UnterHR<-data.frame(Treatment=rbinom(n,1,.4),Female=rbinom(n,1,.1),LeaveJob=0,Race=sample(4,n,prob=c(.6,.1,.1,.2),replace=T))
#Rename race
  UnterHR$Race<-as.factor(ifelse(UnterHR$Race==1,"White",ifelse(UnterHR$Race==2,"Black",ifelse(UnterHR$Race==3,"Latino","Asian"))))
#LeaveJob
  #treatment makes men less likely to leave
    UnterHR$LeaveJob[UnterHR$Treatment==0 & UnterHR$Female==0]<-rbinom(length(UnterHR$LeaveJob[UnterHR$Treatment==0 & UnterHR$Female==0]),1,.3)
    UnterHR$LeaveJob[UnterHR$Treatment==1 & UnterHR$Female==0]<-rbinom(length(UnterHR$LeaveJob[UnterHR$Treatment==1 & UnterHR$Female==0]),1,.2)
  #treatment makes wommen more likely to leave
    UnterHR$LeaveJob[UnterHR$Treatment==0 & UnterHR$Female==1]<-rbinom(length(UnterHR$LeaveJob[UnterHR$Treatment==0 & UnterHR$Female==1]),1,.3)
    UnterHR$LeaveJob[UnterHR$Treatment==1 & UnterHR$Female==1]<-rbinom(length(UnterHR$LeaveJob[UnterHR$Treatment==1 & UnterHR$Female==1]),1,.6)
  #treatment makes blacks and latinos more likely to leave (redraw if leave job = 0 and and black or latino = 1)
    #count people in either category
      n2<-length(UnterHR$LeaveJob[UnterHR$Treatment==1 & UnterHR$LeaveJob==0 & (UnterHR$Race=="Black" | UnterHR$Race=="Latino" )])
    #redraw for those
      UnterHR$LeaveJob[UnterHR$Treatment==1 & UnterHR$LeaveJob==0 & (UnterHR$Race=="Black" | UnterHR$Race=="Latino" )]<-rbinom(n2,1,.5)
      
      
```
*** =sample_code
```{r}
# Before running a regression model, let's examine the data
    str(UnterHR)

# The dataset contains four variables: Treatment, Female, LeaveJob, and race. Notice that the structure command refers to race as a "Factor" variable. This is a typical way that R identifies nominal (non-numeric) variables.
    summary(UnterHR$Treatment)
    summary(UnterHR$Female)
    summary(UnterHR$LeaveJob)
    summary(UnterHR$Race)

# Question 1 asks us construct a regression model that measures the effect of Treatment on LeaveJob, with a "mediation effect"" for Female. This is the same as "controlling" for Female, as we did for other independent variables in our previous examples. For reference, the syntax for glm is "glm(Y~X+Z,data=df)" where Y is the name of the independant variable, X is the name of the dependent variable, Z is the name of the mediation variable, and df is the name of the dataframe.

#---- Question 1-------------------------------------#
      Solution1<-glm()
#----------------------------------------------------#
    
# If you entered Solution1 correctly and summarize the results via summary(Solution1), the effect for Treatment should be about .047 (but non significant), and the effect for Female should be about .16. The direct interpretation of this model is that Treatment increases the odds that a person will leave their job by .046 (4.6%), and being Female increases the odds that a person will leave their job by .16, regardless of Treatment. 
      
# The model in Solution1 does not assume that Treatment effects women differently, but that women have a higher baseline chance of leaving their job, regardless of the treatment effect. In order to find out if the treatment effects women differently, we need to add an interaction between Female and Treatment. For reference, the syntax for glm with an interaction effect is "glm(Y~X*Z,data=df)" where Y is the name of the independant variable, X is the name of the dependent variable, Z is the name of the mediation variable, and df is the name of the dataframe. The difference between the syntax for a mediation and moderation effect is that we multiply X and Z for moderation, rather than add them. Construct a regression model that measures the effect of`Treatment on LeaveJob, with a moderation effect for Female.

#---- Question 2-------------------------------------#
      Solution2<-glm()
#----------------------------------------------------#

# By default, R includes independent variables for Treatment and Female when we add a statistical interaction term. The coefficient for Treatment indicates the effect of Treatment for non-women (men); the coefficient for Female indicates the effect of being a female, independent of Treatment. The Treatment:Female coefficient indicates the effect of Treatment for Females. The large coefficient for this interaction effect suggests that Treatment has a large effect on women's odds of leaving their job.
      
# Let's now answer the final question for this exercise. We need to include an interaction effect between Treatment and Female, and an interaction effect between Treatment and Race. For reference, the syntax for glm with two interaction effects is "glm(Y~X*Z1+X*Z2,data=df)" where Y is the name of the independant variable, X is the name of the dependent variable, Z1 is the name of the first mediation variable, Z2 is the name of the first mediation variable and df is the name of the dataframe.
      
#---- Question 3-------------------------------------#
      Solution3<-glm()
#----------------------------------------------------#
      
      
      
```
*** =solution
```{r}
      Solution1<-glm(LeaveJob ~ Treatment + Female,data=UnterHR)
      Solution2<-glm(LeaveJob ~ Treatment * Female,data=UnterHR)
      Solution3<-glm(LeaveJob ~ Treatment * Female + Treatment * Race,data=UnterHR)
```
*** =sct
```{r}
#test_object("Solution1")
#test_object("Solution2")
#test_object("Solution3")
success_msg("Good work! It appears that the Treatment only effected individuals who were Female, Black, or Latino.")
```



--- type:VideoExercise lang:r aspect_ratio:62.5 xp:50 skills:1 key:7fb39b7cb6
## Logistic Regression Models
*** =video_link
//player.vimeo.com/video/217554577
