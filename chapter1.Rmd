--- 
title       : "Regressions 1: Introduction to Regression As Causality"
description : "This chapter will introduce you to using regression analysis to find causal effects"
 
 
  
--- type:VideoExercise lang:r aspect_ratio:62.5 xp:50 skills:1 key:5c9d6dd3fc
## Introduction to Regression Analysis
*** =video_link
//player.vimeo.com/video/217553687



--- type:MultipleChoiceExercise lang:r xp:50 skills:1 key:1a2137dd19
## Interpreting Regressions
The popular online auctioneer, eGulf, wants to create an algorithm that will automatically provide recommended sales prices to eGulf sellers. In the case of wePhones, eGulf plans to base its algorithm on the age and sales price of previously sold WePhones. For the sake of convenience, eGulf bases their algorithm on a small stratified sample of 25 WePhones sold in the past week. 

eGulf runs an OLS regression to determine the relationship between WePhone age and sales price, and plots their results (illustrated in the R workspace). Each point on the plot indicates the age and selling price for a particular WePhone, and the line shows the results from an OLS regression on this data. What do these results suggest about the relationship of WePhone age and sales price?


*** =instructions
- There is a negative relationship
- There is no relationship
- There is a positive relationship
*** =pre_exercise_code
```{r}
set.seed(1)
library(ggplot2)
WePhone<-data.frame(age=rep(c(1,2,3,4,5),5))
WePhone$Feedback<-round(rnorm(n=25,mean=90,sd=3))
WePhone$Age<-WePhone$age+rnorm(n=25,mean=0,sd=.3)-WePhone$Feedback/10+mean(WePhone$Feedback/10)
WePhone$Value<-500-(9*WePhone$age^2)+ round(rnorm(n=25,mean=0,sd=30))
names(WePhone)[4]<-"Price Sold"
WePhone$`Price Sold`<-WePhone$`Price Sold`+20*(WePhone$Feedback-mean(WePhone$Feedback))
model<-lm(`Price Sold`~Age,data=WePhone)
ggplot(data=WePhone,aes(Age, `Price Sold`))+geom_point()+geom_abline(intercept = model$coefficients[1],slope=model$coefficients[2])+ ggtitle("Scatter Plot and OLS Regression of WePhone Age on Price Sold")
```
*** =sct
```{r}
msg1 = "Correct! As the age of WePhones increased, the price at which they were sold decreased."
msg2 = "Although the plot does not indicate if their results were statistically significant, they do indicate a relationship. Try again"
msg3 = "Think this through carefully. Does an increase in age increase or decrease the price sold of a WePhone? Try again"
test_mc(correct = 1, feedback_msgs = c(msg1,msg2,msg3))
```



--- type:MultipleChoiceExercise lang:r xp:50 skills:1 key:4b9a40fa01
## Reversing Causal Direction
Causal interpretation of regression models can be tricky. Using the same data in the last example, an eGulf analyst decided to toy with a regression model where age is a function of price sold (i.e. price is the independent variable, and age is the dependent variable). The results of this model are visualized in the R workspace. If this model shows a causal relationship between age and price sold, with age as the dependent variable, what **exactly** should the analyst say to describe the results to his boss? 

*** =instructions
- Older phones tend to sell for less
- Old age causes phones to sell for less
- High sales prices causes phones to become younger
*** =pre_exercise_code
```{r}
set.seed(1)
library(ggplot2)
WePhone<-data.frame(age=rep(c(1,2,3,4,5),5))
WePhone$Feedback<-round(rnorm(n=25,mean=90,sd=3))
WePhone$Age<-WePhone$age+rnorm(n=25,mean=0,sd=.3)-WePhone$Feedback/10+mean(WePhone$Feedback/10)
WePhone$Value<-500-(9*WePhone$age^2)+ round(rnorm(n=25,mean=0,sd=30))
names(WePhone)[4]<-"Price Sold"
WePhone$`Price Sold`<-WePhone$`Price Sold`+20*(WePhone$Feedback-mean(WePhone$Feedback))
model<-lm(Age~`Price Sold`,data=WePhone)
ggplot(data=WePhone,aes(`Price Sold`,Age))+geom_point()+geom_abline(intercept = model$coefficients[1],slope=model$coefficients[2])+ ggtitle("Scatter Plot and OLS Regression of WePhone Age on Price Sold")
```
*** =sct
```{r}
msg1 = "This phrasing does not address a causal interpretation of the relationship, but rather is a description of an association between WePhone age and sales price"
msg2 = "This is the most intuitive explanation for the relationship between WePhone sale price and age, but not for this particular regression model, which was designed to use age as the dependent variable. Try again"
msg3 = "Correct! Well, at least according to this model. This is an example of reverse causality. Obviously, we know that in reality the age of any object can only be influenced by time, so we know that this interpretation of their causal relationship is false: price sold cannot cause a phone's age. The analyst should try a different model next time."
test_mc(correct = 3, feedback_msgs = c(msg1,msg2,msg3))
```




--- type:MultipleChoiceExercise lang:r xp:50 skills:1 key:d50636b4ac
## Regression Models and Policy Implementation
Suppose you see a Facebook post with a regression plot of the Number of Walmarts per Capita on Income per Capita, and the slope is positive. If you were asked to advise a town on a policy to attract a Walmart to town, would you say they should give tax refunds to their citizens to increase their income?
*** =instructions
- Yes, the regression shows that Walmarts come to richer places
- No, you can't learn the cause of Walmart locations from the regression
*** =sct
```{r}
msg1 = "Whoops. Try again."
msg2 = "No, you can't learn the direct causal effect from this regression as described, because there are so many other social and economic factors to consider in this relationship that Unconfoundness Assumption is unlikely to be true. In this case, we can just learn the amount of correlation between the variables."
test_mc(correct = 2, feedback_msgs = c(msg1,msg2))
```


--- type:VideoExercise lang:r aspect_ratio:62.5 xp:50 skills:1 key:8bb34ee718
## Basic Elements of a Regression Table
*** =video_link
//player.vimeo.com/video/217554002


--- type:MultipleChoiceExercise lang:r xp:50 skills:1 key:470a340b81
## Reading a Bivariate Regression Table
The following table shows the regression results from the previous questions about eGulf's pricing algorithm for used wePhones. The coefficient for age reflects the slope in the previous line graph, and it suggests that for each additional year (or unit of age), the expected sales price of WePhones decreases by $61.99. The coefficient for the intercept matches the Y-axis intercept in the previous line graph. It indicates the predicted price sold for a WePhone at 0 years old (or more generally, it shows the value of an outcome when all other independent variables are at their reference point). 

| Variable   | Coefficient (SE) |
|------------|-----------------:|
|    Age     |   -61.99 (7.28)  |
| Intercept  |   592.56 (24.42) |

To determine the predicted sales price of a WePhone with a given age, we can start with the initial intercept value and add to it the total change in price for the age of the phone. In other words, based on this regression model the formula for predicting the price of a WePhone sold on eGulf is:

`Sales Price = 592.56 + Age * -61.99`

Based on this formula, what is a valid predicted price for a WePhone that is exactly two years old?



*** =instructions
- 460.49
- 468.58
- 472.57 
- 475.56
*** =sct
```{r}
msg1 = "Try again"
msg2 = "Well done! Notice any advantages of regression models versus t.tests? In a t.test, we can only predict the outcome from a binary treatment variable (that is, the outcome for respondents in the treatment versus control group), In a regression model, we can predict the outcome from a continuous treatment variable (e.g. age)."
msg3 = "Try again"
msg4 = "Try again"
test_mc(correct = 3, feedback_msgs = c(msg1,msg2,msg3,msg4))
```



--- type:MultipleChoiceExercise lang:r xp:50 skills:1 key:9917eaebf2
## Reading a Multivariate (Multiple) Regression Table
In our previous eGulf example, we assumed that age is the only determinant of a WePhone's sales price. However, sellers on eGulf also have "feedback scores," ranging from 0-100, that indicate the quality of their previous transactions with customers. It seems likely that a seller's feedback score could positively impact a WePhone's sales price; that is, customers might be willing to pay more for WePhones sold by reputable sellers. However, if reputable sellers tended to sell newer WePhones, this would confound the relationship between sales price and age. 

We could consider overcoming this by throwing out respondents with high feedback scores that sold new WePhones, so that WePhone age and WePhone feedback score were no longer correlated. That might let us look at the effect of the feedback score in a natural experiment analysis, but throwing out those high scores might imbalance our sample and make that analysis invalid. However, with OLS regression, we can simply add seller feedback scores as another determinant of our independent variable, thereby controlling for its confounding effect on our outcome variable. This is what is shown in the table below.

| Variable   | Coefficient (SE) |
|------------|-----------------:|
|    Age     |  -55.61 (4.94)   |
|  Feedback  |   14.15 (2.55)   |
| Intercept  | -707.23 (234.38) |


As you can see, when controlling for seller score, age has an even larger negative effect on sales. Using this table, calculate the difference (in absolute terms) in the expected sales price for phone that is 2 year old with a feedback score of 80, versus a phone that is 4 years old with a feedback score of 90.


*** =instructions
- 22.85
- 26.83 
- 27.00
- 30.28
*** =sct
```{r}
msg1 = "Try again"
msg2 = "Try again"
msg3 = "Try again"
msg4 = "Well done! The same method can applied with as many variables as we choose. This provides a great way of examining the trade offs between different factors that effect on outcome. However, notice that the intercept is -707.23. That would indicate that a used WePhone with an age of 0 and a feedback score of 0 would have a predicted sales price of -$707.23. Obviously that is unrealistic; no sales on eGulf result in sellers having to pay their customers. This hints at a modeling issue that we'll explore later in the chapter."
test_mc(correct = 4, feedback_msgs = c(msg1,msg2,msg3,msg4))
```


--- type:MultipleChoiceExercise lang:r xp:50 skills:1 key:3b325c1914
## Coefficients of Zero
Does a regression coefficient of 0 with a small confidence interval / small standard errors imply that the outcome and treatment variables simultaneously cause each other?
*** =instructions
- Yes, it shows the variables cancel each other out.
- No, it just shows there's no relationship between these variables in the data.
- It shows "misspecification" and we can't learn anything.

*** =sct
```{r}
msg1 = "It is possible to have simultaneous causality and yet find a regression coefficient of zero, but this is not commonly the reason we see regressions of zero, nor something we could reasonably infer from a coefficient of 0"
msg2 = "Correct! Regressions by themselves are about describing relationships. So a zero regression coefficient shows that there is no relationship in the data between these variables. It does *not* say anything about the causal effect of one variable on the other, unless we make additional assumption, like unconfoundedness."
msg3 = "Not necessarily. We may see a zero coefficient because the model is misspecified, but should not be our immediate conclusion. Moreover, there's almost always something that we can learn from a statistical model; at the very leasy, a statitical model can teach us that we picked the wrong statistical model for our assessing our data."
test_mc(correct = 2, feedback_msgs = c(msg1,msg2,msg3))
```



--- type:VideoExercise lang:r aspect_ratio:62.5 xp:50 skills:1 key:ffa5133d86
## The Relationship between Economic Development and Property Rights
*** =video_link
//player.vimeo.com/video/217554163




--- type:MultipleChoiceExercise lang:r xp:50 skills:1 key:fd77ad9990
## Regressions with small coefficients and small confidence intervals
Does a regression coefficient of 0 with a small confidence interval / small standard errors imply that the outcome and treatment variables simultaneously cause each other?
*** =instructions
- Yes, it shows the variables cancel each other out.
- No, it just shows there's no relationship between these variables in the data.
- It shows "misspecification" and we can't learn anything
*** =sct
```{r}
msg1 = "Actually, OLS regression models can't show simultaneous causality. More advanced methods are needed for this. Try again."
msg2 = "Regressions by themselves are about describing relationships. So a zero regression coefficient shows that there is no relationship in the data between these variables. It does *not* say anything about the causal effect of one variable on the other, unless we make additional assumption, like unconfoundedness. Without extra assumptions, it is possible to have simultaneous causality and yet find a regression coefficient of zero."
msg3 = "Although misspecification might cause a regression model to have no coefficients for variables that should be associated with the outcome of interest, this is not necessarily the case. Moreover, every regression model can teach you something; at the very least, a misspecified regression models can teach us how *not* to specify a regression model for our outcomes of interest."
test_mc(correct = 2, feedback_msgs = c(msg1,msg2,msg3))
```


--- type:MultipleChoiceExercise lang:r xp:50 skills:1 key:f47fad8c5f
## Recalling terminology
Why do we use the words "left hand side" variable and "right hand side" variable in casual inference?

*** =instructions
- It refers to how you graph them on X and Y axis.
- It comes from historical, more qualitative writings.
- It refers to the left and right hand side of an equation.
*** =sct
```{r}
msg1 = "Good try, but remember that on a graph, the Y axis is usually marked on the left side of the page, and the X axis is usually marked on the *bottom* of the page. Try again."
msg2 = "Statistics is a relatively young and quantitative (almost by definition) field. Try again."
msg3 = "When we write down a regression mathematically as an equation, we typically write it as 'Y=beta*X + U' where Y is our outcome variable and X is our treatment or policy variable. Beta here is the regression coefficient and U represents unobserved variables besides X that also affect Y. So 'left hand side' just refers to the left hand side of the equal sign in this equation, while 'right hand side' refers to the right hand side of the equal sign in this equation."
test_mc(correct = 3, feedback_msgs = c(msg1,msg2,msg3))
```


--- type:MultipleChoiceExercise lang:r xp:50 skills:1 key:408e61980c
## Multiple Regression Models
Papers often report several regression models next to each other. This table shows a few truncated models from the example in the video, about the relationship between a country's property rights protections and its GDP. Model 1 shows the coefficient and standard error for the effect of property rights protection on GDP without any control variables. Model 2 shows the effect of property rights protection on GDP while controlling for the country's latitude, and Model 3 shows the same, while also controlling for whether the country is in Asia. Why might the effect of property rights protections on GDP decrease as we add more variables to our model?   

| Variable   | Model 1 | Model 2 | Model 3 |
|------------|--------:|--------:|---------|
| Protection |.52(.06) |.47(.06) |.43(.05) |
| Latitude   |         |.89(.49) |.37(.51) |
| Asia Dummy |         |         |-.62(.19)|


*** =instructions
- Because the inclusion of more variables weakens the statistical power of an independent variable
- Because omitting these variables caused the effect of property rights protection on GDP to be downwardly biased. 
- Because the inclusion of more variables strengthens the statistical power of an independent variable
- Because omitting these variables caused the effect of property rights protection on GDP to be upwardly biased. 
*** =sct
```{r}
msg1 = "Although this is often true, this is not indicated in this figure. If you look at the standard errors, the statistical significance of property rights protections on GDP is relatively small and consistent across models. Try again."
msg2 = "This would mean that the omitted variables confound the relationship between property rights protections and GDP by decreasing the it. Try again"
msg3 = "This is usually not true, nor shown in this figure. Try again."
msg4 = "Correct! Without controlling for these variables, the relationship between property rights protections and GDP seemed larger than it truly was."
test_mc(correct = 4, feedback_msgs = c(msg1,msg2,msg3,msg4))
```



--- type:NormalExercise lang:r aspect_ratio:62.5 xp:50 skills:1 key:
## Running a Regression Model: A Simple Beginning
Dr. Max Funn is interested in understanding the effect of income on happiness. He has a small sample of the General Social Survey (GSS), which asked respondents about their income and happiness independently. Although Dr. Funn predicts that income is positively associated with happiness, he is worried that the relationship between income and happiness might be confounded by their joint relationship with the number of hours worked. But before we get into confounders, let's start with a simpler model that helps Dr. Funn look for signs of any relationship between income and happiness.

Using the dataset `GSS`, help Dr. Max Funn measure the effect of income on happiness by examining the data and running a simplified generalized linear regression model. 
*** =instructions
- 1) Examine the dataframe `GSS`.
- 2) Look at summary statistics of three key variables.
- 3) Look at a simple correlation statistic between income and happiness.
- 4) Construct a regression model that measures the effect of income on happiness.

*** =pre_exercise_code
```{r}

library (dplyr)
library (data.table)
library (purr)

n=1000
set.seed(1)
#Create rnorm function that allows for min and max
  rtnorm <- function(n, mean, sd, min = -Inf, max = Inf){
      qnorm(runif(n, pnorm(min, mean, sd), pnorm(max, mean, sd)), mean, sd)
  }
#Create rounding function that allows to round to numbers above 1
  mround <- function(x,base){ 
          base*round(x/base) 
  } 
#Create scaling function that puts number between 0 and 1
  scale <- function(x){
    (x - min(x))/(max(x)-min(x))
    }

#Dataframe
  GSS<-data.frame(WorkHours=1:n,Income=1:n,Happiness=1:n)
#Work hours 
  GSS$WorkHours<-round(rtnorm(n=n,mean=40,sd=6,min=0,max=55))
#Income (in thousands)
  GSS$Income<-GSS$WorkHours*.5+mround(rtnorm(n=n,mean=30,sd=5,min=0,max=100),.1)
#Scaled variables
  scaledincome<-scale(GSS$Income)
  scaledWorkHours<-scale(GSS$WorkHours)
#Happiness (set as 1:5; rounded to 1; make income 2* as effective as work hours)
  GSS$Happiness<-round(scaledincome-(.5*scaledWorkHours) + rtnorm(n=n,mean=3,sd=.5,min=0,max=5),0)
  
  summary(GSS$WorkHours)
  summary(GSS$Income)
  summary(GSS$Happiness)
    cor(GSS$WorkHours,GSS$Income)
    
```
*** =sample_code
```{r}
# 1) Before running a regression model, let's examine the data in the dataframe `GSS` with the str() function:
#---- Question 1-------------------------------------#
      Solution1<-
#----------------------------------------------------#    

# 2) As you can see, the dataset contains three variables: income (measured in thousands of dollars), number of hours worked, and happiness (measured on five point scale where 1=very unhappy, 2=unhappy, 3=neither happy or unhappy, 4=happy, 5=very happy). In 2 separate statements, run the summary function on both of the variables Income and Happiness to see what the data look like. 
  
# Note: The syntax to run summary() on a single variable is: summary(dataframe$variable), where `dataframe` is the name of the dataframe and `variable` is the name of the variable.

#---- Question 2-------------------------------------#
      Solution2<-
#----------------------------------------------------#    

# 3) Using the correlation function, we can tell that income and happiness are positively correlated. But by how much? What is the effect of increasing one's income by $1000 on their happiness? Let's use the cor() function to see a quantitative measurement of the correlation between the variables Income and Happiness.
  
# Note: The syntax to use the cor() function between two variables is: cor(dataframe$variable1, dataframe$variable2), where `dataframe` is the name of the dataframe and `variable1` and `variable2` are the names of the variables.
      
#---- Question 3-------------------------------------#
      Solution3<-
#----------------------------------------------------#
  
# 4) Finally, let's run our first generalized linear regression model of the effect of income on happiness with the glm() function. 
  
# Note: the syntax for glm is "glm(Y~X,data=dataframe)" where Y is the name of the independent variable, X is the name of the dependent variable, and dataframe is the name of the dataframe.

#---- Question 1-------------------------------------#
      Solution4<-
#----------------------------------------------------#
```
*** =solution
```{r}
    Solution1<-str(GSS)
    Solution2<-summary(GSS$Income)
    summary(GSS$Happiness)
    Solution3<-cor(GSS$Income,GSS$Happiness)
    Solution4<-glm(Happiness~Income,data=GSS)
```
*** =sct
```{r}
test_object("Solution1")
test_object("Solution2")
test_object("Solution3")
test_object("Solution4")
success_msg("Good work!")
```


--- type:NormalExercise lang:r aspect_ratio:62.5 xp:50 skills:1 key:
## Running a Regression Model: Improving Our Model
As we mentioned in the previous question, Dr. Funn predicts that income is positively associated with happiness, he is worried that the relationship between income and happiness might be confounded by their joint relationship with work hours. Specifically, Dr. Funn believes that people who work more hours make more money (which indirectly increases a person's happiness), but that people who work more hours at the same income tend to be less happy (i.e. work hours has a direct negative effect on happiness). This would mean that we can't trust our first model to give us accurate results.

So now using the dataset `GSS`, help Dr. Max Funn measure the effect of income on happiness, **controlling** for work hours. 

*** =instructions
- 1) Examine the results of our simple first model from the previous question.
- 2) Plot the relationship between happiness and income, including a regression line.
- 3) Look at correlation statistics for confounders between work hours, income, and happiness.
- 4) Construct a second regression model that measures the effect of income on happiness that controls for work hours.

*** =pre_exercise_code
```{r}

library (dplyr)
library (data.table)
library (purr)

n=1000
set.seed(1)
#Create rnorm function that allows for min and max
  rtnorm <- function(n, mean, sd, min = -Inf, max = Inf){
      qnorm(runif(n, pnorm(min, mean, sd), pnorm(max, mean, sd)), mean, sd)
  }
#Create rounding function that allows to round to numbers above 1
  mround <- function(x,base){ 
          base*round(x/base) 
  } 
#Create scaling function that puts number between 0 and 1
  scale <- function(x){
    (x - min(x))/(max(x)-min(x))
    }

#Dataframe
  GSS<-data.frame(WorkHours=1:n,Income=1:n,Happiness=1:n)
#Work hours 
  GSS$WorkHours<-round(rtnorm(n=n,mean=40,sd=6,min=0,max=55))
#Income (in thousands)
  GSS$Income<-GSS$WorkHours*.5+mround(rtnorm(n=n,mean=30,sd=5,min=0,max=100),.1)
#Scaled variables
  scaledincome<-scale(GSS$Income)
  scaledWorkHours<-scale(GSS$WorkHours)
#Happiness (set as 1:5; rounded to 1; make income 2* as effective as work hours)
  GSS$Happiness<-round(scaledincome-(.5*scaledWorkHours) + rtnorm(n=n,mean=3,sd=.5,min=0,max=5),0)
  
  summary(GSS$WorkHours)
  summary(GSS$Income)
  summary(GSS$Happiness)
  cor(GSS$WorkHours,GSS$Income)
    
#Importing Answer from Previous Question
    FirstModel<-glm(Happiness~Income,data=GSS)
```
*** =sample_code
```{r}
# 1) The glm function in R usually provides less information about the model than desired, and usually in a less than ideal format. So let's use the summary function on our first model to see its effects more clearly. The dataframe containing our first model's results is called `FirstModel`. 
#---- Question 1-------------------------------------#
      Solution1<-
#----------------------------------------------------#
    
# The table beneath "Coefficients" is what is usually reported in scientific journals. "Estimate" refers to the coefficients for each parameter, "Pr(>|t|)" refers to the p.value of the estimates, and the stars following p.value refer to the result's statistical significance. 
    
# If you entered Solution1 correctly, the effect of Income should be about .015, meaning that a single unit increase in income ($1000) is associated with a .015 increase in a person's reported happiness. The low p.value and stars to the right of the coefficients suggest that this effect is statistically significant. 

# 2) This relationship can also be viewed graphically. Let's use the plot() and abline() functions together to generate a graph of the relationship between the variables Happiness and Income while also showing a regression line from Solution1. First run the plot function, then run abline with the following syntax: abline(Solution1).
  
# Note: the syntax for the plot() command will be plot(dataframe$Y ~ dataframe$X), where Y is the name of the independent variable, X is the name of the dependent variable, and dataframe is the name of the dataframe. 

#---- Question 2-------------------------------------#
      Solution2<-
#----------------------------------------------------#    
    
# The points on the plot represent each observation, and the diagonal line represents the regression results. Note that the slope of the line is equal to the coefficient for income.
    
# 3) Now let's look more closely at the correlations of work hours on happiness and income. In 2 separate statements, use the cor() command to find the correlation between the variables WorkHours and Happiness, and between the variables WorkHours and Income.
    
#---- Question 3-------------------------------------#
      Solution3<-
#----------------------------------------------------#

# These results show that work hours are negatively correlated with happiness and positively correlated with income. This suggests that work hours might be confounding the relationship between income and happiness.    
    
# 4) So let's solve that by controlling for the effect of work hours on happiness. You can do this for yourself easily in R by adding the control variable directly to the glm function to regress the effect of income on happiness. 
    
# Note: the syntax for glm with controls is "glm(Y~X+Z,data=dataframe)" where Y is the name of the independent variable, X is the name of the dependent variable, Z is the name of the control variable, and dataframe is the name of the dataframe.

#---- Question 2-------------------------------------#
      Solution2<-glm()
#----------------------------------------------------#
  
```
*** =solution
```{r}
    Solution1<-summary(FirstModel)
    Solution2<-plot(GSS$Happiness ~ GSS$Income)
    abline(Solution1)
    Solution3<- cor(GSS$WorkHours,GSS$Happiness)
    cor(GSS$WorkHours,GSS$Income)
    Solution4<-glm(Happiness~Income+WorkHours,data=GSS)
```
*** =sct
```{r}
test_object("Solution1")
test_object("Solution2")
test_object("Solution3")
test_object("Solution4")
success_msg("Good work! You may have noticed that the coefficient for income increased when we controlled for WorkHours. This is typical when a control variable is positively correlated with both the independent and dependent variables. This syntax can be used to control for as many variables as desired in a regression model.")
```




--- type:MultipleChoiceExercise lang:r xp:50 skills:1 key:9e19547f67
## Steps prior to analysis with regression models 
Which of the following is the most important step you should take before running a regression model on your data?
*** =instructions
- Identify how many variables you can include in your model while still attaining statistical significance
- Identify your outcome of interest (i.e. depend variable)
- Identify which type of regression model to perform
- Identify whether any of your variables will need to be transformed
*** =sct
```{r}
msg1 = "This is actually one of the later steps to take when building a regression model. It is usually best to build simple models before adding numerous control variables. Try again."
msg2 = "Correct! This answer may seem obvious, but its importance cannot be understated.  Identifying one's outcome of interest is often the most frequently skipped step among newcomers to statistical analysis. All statistical analysis should be guided by theory about the causal relationship between an outcome of interest and its antecedents. Having a clear plan about what relationship you plan to test is extremely helpful in determining how to specific your statistical model. Conversely, mining through data without any clear theory guiding your research rarely leads to fruitful or interesting findings. "
msg3 = "This is an important step, but it is not the most important of your options. Try again."
msg4 = "Depending on the variables in you plan to use in your model, this is not always an important, much less necessary, step when building your model. Try again."
test_mc(correct = 2, feedback_msgs = c(msg1,msg2,msg3,msg4))
```
