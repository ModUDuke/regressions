---
title: 'Regressions 2: Using Regression to Estimate Causal Effects'
description: 'This chapter will introduce you to using regression analysis to find causal effects'
---

## Using Regression to Get Causal Effects: Unconfoundedness

```yaml
type: VideoExercise
key: d75a4ac636
lang: r
xp: 50
skills: 1
video_link: //player.vimeo.com/video/217554416
```


---

## Regressing Soda pop and K-pop

```yaml
type: MultipleChoiceExercise
key: 36681d6cec
lang: r
xp: 50
skills: 1
```

You are studying the effect of pop songs on teenage product consumption using a dataset from South Korea. Via a regression model, you find that for every four songs downloaded, soft drink sales tend to increase by about one can. Does this mean that South Korean soda companies will increase their sales if they start sponsoring pop stars?

`@instructions`
- No
- Yes

`@sct`
```{r}
msg1 = "Correct. The regression model does not necessarily recover a true causal effect of song downloads on soft drink sales."
msg2 = "Whoops. Try again."
test_mc(correct = 1, feedback_msgs = c(msg1,msg2))
```

---

## Knowing it all

```yaml
type: MultipleChoiceExercise
key: 5c60df0d16
lang: r
xp: 50
skills: 1
```

If you could observe every single variable that mattered for determining outcomes, would you be able to learn all the causal effects you want?

`@instructions`
- No, regressions only give you correlation, not causation.
- Yes, you'd have every combination of variables possibly needed for determining causality.
- Yes, but just for that dataset.
- No, it would give you conflicting information.

`@sct`
```{r}
msg1 = "Although it may seem like that is what this course is implying, there are times when regressions can demonstrate a causal relationship. Try again."
msg2 = "According to our definition of causality, if you truly observed every relevant variable, you would indeed be able to learn all the causal effects you want. There are a few caveats though. First, you would still need to ensure that there was at least some variation in each variable you cared about. Even if you observed every variable that mattered, if you never see those variables change across units, you will not be able to learn their causal effects. Second, no real world dataset in social science will truly have all variables you want, which is why the unconfoundedness assumption is often made-to assume that the variables we do not observe are not related to the treatment."
msg3 = "This answer is tricky. Remember, the dataset contains every relevent variable in the universe for determining outcomes. That should make findings from it generalizable beyond the dataset. Try again."
msg4 = "It's true that you might have a hard time finding statistically significant results if your sample was small, as there theoretically could be an infinite number of relevant variables for any outcome in the universe, many of which are correlated. However, many events do have a variety of causes, and many regression models do incorporate a lot of 'conflicting' variables. Try again."
test_mc(correct = 2, feedback_msgs = c(msg1,msg2,msg3,msg4))
```

---

## The Uncounfoundedness Assumption

```yaml
type: MultipleChoiceExercise
key: ff6e381314
lang: r
xp: 50
skills: 1
```

Although regression models are great for summarizing the association between variables, any causal interpretation of a regression models rests on the uncounfoundedness assumption (also known as the selection-on-observables assumption; and sometimes referred to as the conditional independence assumption). Which of the following summarizes this assumption?

`@instructions`
- All variables that affect treatment assignment and the outcome of interest are observable and conditioned upon.
- If something happens more frequently than expected during a given period, we can expect that it will happen less frequently in the future
- There is little or no multicollinearity in the data
- The residuals of a regression model are normally distributed.

`@sct`
```{r}
msg1 = "Correct. Put even more simply, any causal interpretation of a regression model assumes that the relationships observed in the data are unconfounded. In most cases, this is impossible to verify empirically. However, it is a pretty large assumption. Most variations of regression models that we will describe in later chapters try to reduce the strength of this assumption."
msg2 = "This is actually a summary of the gambler's fallacy. Try again"
msg3 = "This is an assumption that regression models make, but it is not a description of the unconfoundedness assumption"
msg4 = "This is an assumption that regression models make, but it is not a description of the unconfoundedness assumption"
test_mc(correct = 1, feedback_msgs = c(msg1,msg2,msg3,msg4))
```

---

## How to Compute Regressions: Ordinary Least Squares (OLS)

```yaml
type: VideoExercise
key: 9b0c9370cf
lang: r
xp: 50
skills: 1
video_link: //player.vimeo.com/video/217557161
```


---

## Toying with OLS I: Outliers

```yaml
type: NormalExercise
key: 1ff9f6d33f
lang: r
xp: 100
skills: 1
```

Although the math behind determining an OLS regression model is complicated, understanding what it's trying to do is surprisingly straightforward. Understanding how regression models work can yield important insight into understanding how they can be biased. In this question, we will examine how outliers can effect regression slopes. Using data provided by the popular online auctioneer eGulf about the relationship between WePhone age and sales price, follow the sample code to answer the following question about outliers:

`@instructions`
- 1) Refresh our memories on what the eGulf data looks like.
- 2) Run a glm model on how a phone's age affects its sales price on eGulf.
- 3) Examine a graph of this relationship.
- 4) Examine the impact of outliers at different phone ages through graphs.
- 5) Decide when outliers will have the least impact in this regression.

`@hint`
- The syntax for glm is "glm(Y~X,data=df)" where Y is the name of the independent variable, X is the name of the dependent variable, and df is the name of the dataframe.

`@pre_exercise_code`
```{r}
set.seed(1)
library(ggplot2)
WePhone<-data.frame(age=rep(c(1,2,3,4,5),4))
WePhone$Feedback<-round(rnorm(n=20,mean=90,sd=3))
WePhone$Age<-WePhone$age+rnorm(n=20,mean=0,sd=.3)-WePhone$Feedback/10+mean(WePhone$Feedback/10)
WePhone$Value<-500-(9*WePhone$age^2)+ round(rnorm(n=20,mean=0,sd=30))
names(WePhone)[4]<-"Price Sold"
WePhone$`Price Sold`<-WePhone$`Price Sold`+20*(WePhone$Feedback-mean(WePhone$Feedback))
WePhone$age<-NULL
WePhone$Feedback<-NULL
names(WePhone)[2]<-"PriceSold"
```

`@sample_code`
```{r}
# 1) As a quick refresher, let's glimpse at the first few lines of data. In the dataframe `WePhone`, Age refers to the number of years old the WePhone was, and Price Sold refers to the dollars it sold for. Use the head() function to look at these initial lines:



# It looks like these phones ranged between 1-6 years old, and they sold for about $180-$620. You might guess that the newest phones should sell for more money, so let's look at that correlated the Age and PriceSold variables are.


# 2) We will use a regression model to show us the correlation between a WePhone's Age and Price Sold across all the rest of the dataset. Run a glm model named `model` of Age on PriceSold using the dataframe `WePhone`, and then run the summary() command on that model to look at the results.
  
      model<-
      summary(model)

# These results show that the correlation we saw in the first few results from the head() command stay true through the rest of the data. These things are often easier to understand through a graph, so let's look at that next.
    
# 3) Below visualizes the regression model as a line, and the data points as dots. The line that it (and any regression model) produces minimizes the vertical distance (squared) between each data point and the regression line.  We've already generated the code for this graph, so select both of the following lines and hit the "Run Code" button to see the graph:

    plot(WePhone$PriceSold ~ WePhone$Age)
    abline(model)
    
# It looks like there's a pretty consistent negative correlation between age and the price sold, which makes sense. As phones get older, they are worth less money.


# 4) But what happens when we introduce outliers in the data? The following examples show what happen to the regression line when we insert an observation with an unusually high price sold ($1500) given its age at three points of the graph, when Age = 1, 3, and 5. Select each and hit "Run Code", paying attention to the small differences in the outputs as we change the options.
  # No outlier
    plot(WePhone$PriceSold[1:20] ~ WePhone$Age[1:20], ylim=c(0, 1600))
    abline(glm(PriceSold~Age,data=WePhone[1:20,]))
  # Outlier at Age = 1
    WePhone[21,]<-c(1,1500)
    plot(WePhone$PriceSold ~ WePhone$Age, ylim=c(0, 1600))
    abline(glm(PriceSold~Age,data=WePhone))
  # Outlier at Age = 3
    WePhone[21,]<-c(3,1500)
    plot(WePhone$PriceSold ~ WePhone$Age, ylim=c(0, 1600))
    abline(glm(PriceSold~Age,data=WePhone))
  # Outlier at Age = 5
    WePhone[21,]<-c(5,1500)
    plot(WePhone$PriceSold ~ WePhone$Age, ylim=c(0, 1600))
    abline(glm(PriceSold~Age,data=WePhone))


# 5) Now that you have seen the graphs change as we introduced outliers, answer the following question. When does an outlier in a regression model's dependent variable, "Y", causes the LEAST amount of bias (or change in slope) in its relationship to an independent variable, "X"? Answer the solution with "A", "B", or "C".

# (A) When the outlier in Y occurs at low values of X
# (B) When the outlier in Y occurs at middle values of X
# (C) When the outlier in Y occurs at high values of X

      Solution5<-""
```

`@solution`
```{r}
    head(WePhone)
    model<-glm(PriceSold~Age,data=WePhone)
    summary(model)
    Solution5<-"B"
```

`@sct`
```{r}
test_object("Solution5")
test_error()
success_msg("Good work! Outliers typically effect the slope of a bivariate regression line least when they occur at middle values of X. When an outlier significantly alters the slope of a regression line, we refer to it as an 'influence point.' These typically need to be dealt with or removed to produce reliable regression results (i.e. a single outlier could mask the entire relationship between two variables). As a caveat, it should be noted that outliers do not always bias bivariate relationships (e.g. when an unusally high value of Y occurs with an unusually high value of X), but such outliers tend to have great statistical 'leverage,' which can lead to substantial bias in multivariate relationships (such as when a regression model includes more than one key independent variable). ")
```

---

## Toying with OLS II: Statistical Power

```yaml
type: NormalExercise
key: 4bb956183e
lang: r
xp: 100
skills: 1
```

Since OLS models are so easy to visualize, they provide a nice tool for understanding why models that use more observations tend to have more statistical power. In the following question, we have two datasets provided by the popular online auctioneer eGulf about the relationship between WePhone age and sales price, one with 25 observations. For each of these datasets, we add 25 randomly generated observations that have no relationship between WePhone age and sales price, to represent "statistical noise" - unexplained variation in our sample. Follow the sample code to create regression models with these two datasets, and answer the following question:

`@instructions`
- 1) Take a look at summary statistics of the two WePhone datasets.
- 2) Build glm models that estimate the effect of phone ages on sales prices in both datasets.
- 3) Which regression model most accurately represents the true relationship between WePhone `Age` and `SalesPrice`?

`@pre_exercise_code`
```{r}
#Original dataset
set.seed(1)
library(ggplot2)
WePhone<-data.frame(age=rep(c(1,2,3,4,5),5))
WePhone$Feedback<-round(rnorm(n=25,mean=90,sd=3))
WePhone$Age<-WePhone$age+rnorm(n=25,mean=0,sd=.3)-WePhone$Feedback/10+mean(WePhone$Feedback/10)
WePhone$Value<-500-(9*WePhone$age^2)+ round(rnorm(n=25,mean=0,sd=30))
names(WePhone)[4]<-"Price Sold"
WePhone$`Price Sold`<-WePhone$`Price Sold`+20*(WePhone$Feedback-mean(WePhone$Feedback))
WePhone$age<-NULL
WePhone$Feedback<-NULL
names(WePhone)[2]<-"PriceSold"
WePhone1<-WePhone

#Second dataset
WePhone<-data.frame(age=rep(c(1,2,3,4,5),20))
WePhone$Feedback<-round(rnorm(n=100,mean=90,sd=3))
WePhone$Age<-WePhone$age+rnorm(n=100,mean=0,sd=.3)-WePhone$Feedback/10+mean(WePhone$Feedback/10)
WePhone$Value<-500-(9*WePhone$age^2)+ round(rnorm(n=100,mean=0,sd=30))
names(WePhone)[4]<-"Price Sold"
WePhone$`Price Sold`<-WePhone$`Price Sold`+20*(WePhone$Feedback-mean(WePhone$Feedback))
WePhone$age<-NULL
WePhone$Feedback<-NULL
names(WePhone)[2]<-"PriceSold"
WePhone2<-WePhone

#Random noise
noise<-data.frame(Age=c(1,2,3,4,5)+rnorm(n=25,mean=0,sd=.3))
noise$PriceSold<-round(rnorm(n=25,mean=400,sd=100))

#Append random noise
WePhone1<-rbind(WePhone1,noise)
WePhone2<-rbind(WePhone2,noise)
rm(list = c("noise","WePhone"))
```

`@sample_code`
```{r}
# 1) We have two datasets, WePhone1 and WePhone2. The first contains 25 real observations and 25 random observations; the second contains 100 real observations and 25 random observations. As a reminder, each dataset contains two variables, "Age"" and "PriceSold." Run the summary command on both `WePhone1` and `WePhone2` to see what we have:



# 2) On the surface, these datasets look quite similar. But how do their different sample sizes affect estimates from regression models? Build regression models using the glm function that shows the effect of Age on PriceSold for WePhones in each dataset.

# glm in dataset WePhone1
    
        Solution2<-glm()

# glm in dataset WePhone2
    
        Solution3<-glm()

#Note: We should view the output of these models with the summary function to answer the next question.
        summary(Solution2)
        summary(Solution3)
        
# 3) Although the estimates in Solution2 and Solution3 are similar and are both statistically significant, these models arrive at very different standard errors (a core measurement of the accuracy of our estimates). Larger standard errors indicate that we are less certain of our estimates. Which of the two models' estimates are we more certain in, "Solution2" or "Solution3"?
      
        Solution4<-""
      
# Note: For fun, we can plot these models on a graph with error bars using the smooth function in ggplot. In this case, the error bars represent the range of values that we are 95% sure the true population estimate lies between. Again greater error bars indicate that we have less certainty about the true relationship of our variables.
      
ggplot(NULL, aes(x=Age,y=PriceSold))+
    theme_bw()+
    geom_smooth(data=Solution2, color="red",fill="red",method='lm')+
    geom_smooth(data=Solution3, color="blue",fill="blue",method='lm')
      
```

`@solution`
```{r}
summary(WePhone1)
summary(WePhone2)
Solution2<-glm(PriceSold ~ Age, data=WePhone1)
Solution3<-glm(PriceSold ~ Age, data=WePhone2)
Solution4<-"Solution3"
```

`@sct`
```{r}
test_object("Solution2")
test_object("Solution3")
test_object("Solution4")
test_error()
success_msg("Good work! Larger datasets tend to be more robust to 'statistical noise' - unexplained variation in the sample. In this example, statistical noise decreased the relationship between Age and PriceSold (i.e. the coefficient of Age in the model). However, it is possible for statistical noise to induce spurious relationships as well.")
```

---

## Toying with OLS III: Model Selection

```yaml
type: NormalExercise
key: d4a11a1bf2
lang: r
xp: 100
skills: 1
```

NixSplash, a water conservationist group in Utah, wants to make a targeted ad campaign to encourage high water consumers to reduce their water usage. To determine who to target with their ad campaign, NixSplash conducts a survey across a random sample of the population. However, the intern who put together NixSplash's survey asked a bunch of odd questions. Use NixSplash's survey dataset, `Survey`, to generate the best model for predicting a household's water usage.

  *** =instructions
- 1) Examine the data from the survey
- 2) Build a regression model that includes all statistically significant predictors of a household's water usage (`Water`).
- 3) Build a regression model that includes all intuitive predictors of a household's water usage (`Water`).
- 4) Decide which way of modeling is better.

`@pre_exercise_code`
```{r}
set.seed(1)
n=1337
#Create rnorm function that allows for min and max
rtnorm <- function(n, mean, sd, min = -Inf, max = Inf){
  qnorm(runif(n, pnorm(min, mean, sd), pnorm(max, mean, sd)), mean, sd)
}
#Create rounding function that allows to round to numbers above 1
mround <- function(x,base){ 
  base*round(x/base) 
} 
#Dataset
  Survey<-data.frame(YardSize=round(rtnorm(n,.2,sd=.1,min=.05,max=5),2),
                     Rainfall=round(rtnorm(n,8,sd=1,min=4,max=12),1),
                     NSprinkler=rbinom(n,1,.2),
                     Dunkin=rbinom(n,1,.15),
                     Massage=rbinom(n,1,.05),
                     Coffee=rbinom(n,1,.35)
                     )
  Survey$Water<-(Survey$YardSize*10+Survey$Rainfall+Survey$NSprinkler*5+rnorm(n,10,4))*15
```

`@sample_code`
```{r}
# Note: Here is a data dictionary about the variables:
#    Water refers to the number of gallons (in hundreds) that the respondent has used in the past year
#    YardSize refers to the number of acres that the respondent owns
#    Rainfall refers to the number of annual inches in rain that fell on the respondent's property in the past year
#    NSprinkler refers to whether the respondent's neighbor runs a sprinkler system on a daily basis
#    Dunkin refers to whether the respondent lives within walking distance of a Dunkin' Donuts
#    Massage refers to whether the respondent has ever gotten a massage
#    Coffee refers to whether the respondent drinks coffee on a daily basis. 

# 1) As usual, let's first examine the dataset. Start by running the summary() command on the dataframe `Survey`:

    summary()

# 2) Some of the survey questions may seem irrelevant to how much water a household uses, but let's find out for certain. Let's first run an OLS regression model for water usage while using all variables in the dataset. 
  
# Note: We can find the summary statistics from a generalized linear regression model by inserting the glm() code as the parameter for the summary() command, like the following: summary(glm(Water ~ YardSize, data=Survey)). Now do that for yourself, but include all of the variables in the dataframe:



# If you did this correctly, you should see a statistically significant effect from all variables except Dunkin and Massage.


# 3) Since the variables Dunkin and Massage have no significant effect on Water, let's try removing them from our model and see what changes. Enter the summary(glm()) code with these variables omitted:



# So which model is better? A model that predicts water usage with all of our available variables, or a model that predicts water usage without Dunking or Coffee? It turns out that statisticians have developed a variety of tools to measure how well a model specification fits the data (i.e. a model's "goodness of fit"). The GLM function includes one such parameter, its "AIC" (Akaike information criterion). The math behind AICs is complicated, but in practice, a model's AIC can be used to compare the goodness of fit between two models. A model with a lower AIC than another is considered to fit the data better. 


# 4) In the above example, the AIC of our second model is lower than in our first example, but only very slightly ( a rule of thumb is that a difference of 5 is considered substantial). This suggests that the models fit the data more or less equally well. So which should we choose? Should NixSplash incorporate some of our unintuitive variables into their model and ad-campaign? In other words, should we include all variables in our model even if it is unclear why proximity to Dunkin' Donuts, Massage frequency, and Coffee-drinking habits would effect water useage? Answer Solution 4 with "Yes" or "No".

      Solution4<-""
```

`@solution`
```{r}
summary(Survey)
summary(glm(Water~YardSize+Rainfall+NSprinkler+Rainfall+Dunkin+Massage+Coffee,data=Survey))
summary(glm(Water~YardSize+Rainfall+NSprinkler+Rainfall+Coffee,data=Survey))
Solution4<-"No"
```

`@sct`
```{r}
test_object("Solution4")
test_error()
success_msg("Good work! Even though our model in Solution 1 appears similar to our model in Solution2,  we should **not** include all of these variables into our final model, or use them to inform NixSplash's ad-campaign. Statistical models should always be guided by theory. If there is no clear reason why a variable should effect an outcome of interest, we should not include it in our model, even if it improves model fit. As a reminder, statistical models do not provide definitive proof of causality; they are simply a tool that we can use to test our assumptions about the world.")
```

---

## Common Statistical Terms and Transformations in Regression Models

```yaml
type: VideoExercise
key: 93014adc4b
lang: r
xp: 50
skills: 1
video_link: //player.vimeo.com/video/233015981
```


---

## Idenfifying Non-Linear Relationships

```yaml
type: MultipleChoiceExercise
key: 4b9a40fa01
lang: r
xp: 50
skills: 1
```

We have generated 4 different plots, which you can scroll through in the Plots display. In which of those plots shows a non-linear relationship betweeen X and Y?

`@instructions`
- A and B
- C and D
- B and C
- B

`@pre_exercise_code`
```{r}
seed=1

n=500
df<-data.frame(x=rnorm(n=n,mean=50,sd=25))
df$y1=df$x+(rnorm(n,100,10))
df$y2=120-df$x+(rnorm(n,100,10))
df$y3=log10(df$x)+rnorm(n=n,mean=.5,sd=.2)
df$y4=(df$x^3)/10000+rnorm(n,50,10)

plot(df$x,df$y1)+title(main="Plot A")
plot(df$x,df$y3)+title(main="Plot B")
plot(df$x,df$y4)+title(main="Plot C")
plot(df$x,df$y2)+title(main="Plot D")
```

`@sct`
```{r}
msg1 = "Plot A is linear. Try again."
msg2 = "Plot D is linear. Try again"
msg3 = "Correct! The relationship between X and Y in Plot C is logarithmic, and the relationship between X and Y in Plot D is exponential. If you modeled these relationships with OLS regression, you would want to include a squared or logarithmic term."
msg4 = "Close, but Plot B is not the only plot that illustrates a non-linear relationship."
test_mc(correct = 3, feedback_msgs = c(msg1,msg2,msg3,msg4))
```

---

## Statistical Interactions in Regression Models

```yaml
type: VideoExercise
key: 7bdd9c55dc
lang: r
xp: 50
skills: 1
video_link: //player.vimeo.com/video/233015807
```


---

## Creating a Regression Model With Interaction Effects: Part 1

```yaml
type: NormalExercise
key: 99c76a856a
lang: r
xp: 100
skills: 1
```

An experiment conducted by the transportation network company, Unter Technologies, tried to determine whether downsizing their Human Resources (HR) department would lead to higher employee turnover. Using t.tests, they found that the effect of downsizing their HR department would have different conditional average treatment effects (CATE) on men and women. This was determined by separately studying the average treatment effect (ATE) of downsizing on a sample of men and on a sample of just women. 

However, slicing data this way can substantially reduce one's statistical power, and becomes unwieldy when a data scientist wants to determine more than a couple of CATEs with a given sample. For example, if race is also and important factor in whether employees plan to leave Unter if Unter reduces their HR department, we would need to slice the data into smaller samples several more times, and run several distinct t.tests on the data. One of the major benefits of regression models is that we can use all of the data points to find an answer, but it also means our interpretations can be more open to debate. Let's see what a regression model tells us about the treatment effect in this experiment.

`@instructions`
- 1) Examine the structure of the dataframe `UnterHR`
- 2) Look at summary statistics for each variable
- 3) Construct a regression model that measures the effect of `Treatment` on `LeaveJob`, with a mediation effect for `Female`

`@hint`
- The syntax for glm is "glm(Y~X+Z,data=df)" where Y is the name of the independent variable, X is the name of the dependent variable, Z is the name of the mediating variable, and df is the name of the dataframe.

`@pre_exercise_code`
```{r}
set.seed(1)
n=682
#Create Dataframe
  UnterHR<-data.frame(Treatment=rbinom(n,1,.4),Female=rbinom(n,1,.1),LeaveJob=0,Race=sample(4,n,prob=c(.6,.1,.1,.2),replace=T))
#Rename Race
  UnterHR$Race<-as.factor(ifelse(UnterHR$Race==1,"White",ifelse(UnterHR$Race==2,"Black",ifelse(UnterHR$Race==3,"Latino","Asian"))))
#LeaveJob
  #treatment makes men less likely to leave
    UnterHR$LeaveJob[UnterHR$Treatment==0 & UnterHR$Female==0]<-rbinom(length(UnterHR$LeaveJob[UnterHR$Treatment==0 & UnterHR$Female==0]),1,.3)
    UnterHR$LeaveJob[UnterHR$Treatment==1 & UnterHR$Female==0]<-rbinom(length(UnterHR$LeaveJob[UnterHR$Treatment==1 & UnterHR$Female==0]),1,.2)
  #treatment makes wommen more likely to leave
    UnterHR$LeaveJob[UnterHR$Treatment==0 & UnterHR$Female==1]<-rbinom(length(UnterHR$LeaveJob[UnterHR$Treatment==0 & UnterHR$Female==1]),1,.3)
    UnterHR$LeaveJob[UnterHR$Treatment==1 & UnterHR$Female==1]<-rbinom(length(UnterHR$LeaveJob[UnterHR$Treatment==1 & UnterHR$Female==1]),1,.6)
  #treatment makes blacks and latinos more likely to leave (redraw if leave job = 0 and and black or latino = 1)
    #count people in either category
      n2<-length(UnterHR$LeaveJob[UnterHR$Treatment==1 & UnterHR$LeaveJob==0 & (UnterHR$Race=="Black" | UnterHR$Race=="Latino" )])
    #redraw for those
      UnterHR$LeaveJob[UnterHR$Treatment==1 & UnterHR$LeaveJob==0 & (UnterHR$Race=="Black" | UnterHR$Race=="Latino" )]<-rbinom(n2,1,.5)
```

`@sample_code`
```{r}
# 1) Before running a regression model, let's examine the structure of the dataframe `UnterHR` with the str() command:



# 2) The dataset contains four variables: Treatment, Female, LeaveJob, and Race. Notice that the structure command refers to race as a "Factor" variable. This is a typical way that R identifies nominal (non-numeric) variables. In 4 separate statements, look at the summary statistics about each variable:






# 3) Remember that Unter found that that downsizing their HR department had a bigger CATE for women than for men, so let's construct a regression model that measures the effect of Treatment on LeaveJob, with a "mediation effect"" for Female. This is the same as "controlling" for Female, as we did for other independent variables in our previous examples. Then run the summary() command to further understand the results.
  
    Solution3<-
```

`@solution`
```{r}
str(UnterHR)
summary(UnterHR$Treatment)
    summary(UnterHR$Female)
    summary(UnterHR$LeaveJob)
    summary(UnterHR$Race)
Solution3<-glm(LeaveJob ~ Treatment + Female,data=UnterHR)
    summary(Solution3)
```

`@sct`
```{r}
test_object("Solution3")
test_error()
success_msg("Good work! If you entered Solution3 correctly and summarize the results via summary(), the effect for Treatment should be about .046 (but not statistically significant), and the effect for Female should be about .16. The direct interpretation of this model is that Treatment increases the odds that a person will leave their job by .046 (4.6%), and being Female increases the odds that a person will leave their job by .16, regardless of Treatment.")
```

---

## Creating a Regression Model With Interaction Effects: Part 2, Mediating and Moderating Effects

```yaml
type: NormalExercise
key: 3f490a5c1c
lang: r
xp: 100
skills: 1
```

A CATE is basically an example of **statistical moderation** (also known as an interaction effect), where the effect of an independent variable is moderated by the effect of a second independent variable. A good example of moderation on a causal effect could be seen in your sink faucets. When your water valves are shut, water does not come out, but when you open your valves, water comes out. The valves are not the direct cause of water come out of your pipes - pressure is - but the valves **moderate** the relationship between the pressure in your pipes and your sink. 

In other words, statistical moderation occurs when the size of one independent variable's effect on an outcome is affected by a second independent variable. In this example, we will find that gender moderates the effect of the treatment (downsizing HR) on someone's intention to leave Unter Technology. With the dataframe, `UnterHR`, construct three regression models: One that naively estimates the average treatment effect of reducing the size of Unter's HR department on employee turnover, a second that includes a statistical interaction (to allow for moderation) between treatment and gender (`Female`), and a third that includes interactions between treatment and gender (`Female`) and treatment and race (`Race`).

`@instructions`
- 1) Construct a regression model that measures the effect of `Treatment` on `LeaveJob`, mediated by `Female`
- 2) Construct a regression model that measures the effect of `Treatment` on `LeaveJob`, mediated by `Female` and with an interaction effect between `Treatment` and `Female`
- 3) Construct a regression model that measures the effect of `Treatment` on `LeaveJob`, mediated by `Female` and Race, with interaction effects between `Treatment` and `Female` and between `Treatment` and `Race`.

`@pre_exercise_code`
```{r}
set.seed(1)
n=682
#Create Dataframe
  UnterHR<-data.frame(Treatment=rbinom(n,1,.4),Female=rbinom(n,1,.1),LeaveJob=0,Race=sample(4,n,prob=c(.6,.1,.1,.2),replace=T))
#Rename race
  UnterHR$Race<-as.factor(ifelse(UnterHR$Race==1,"White",ifelse(UnterHR$Race==2,"Black",ifelse(UnterHR$Race==3,"Latino","Asian"))))
#LeaveJob
  #treatment makes men less likely to leave
    UnterHR$LeaveJob[UnterHR$Treatment==0 & UnterHR$Female==0]<-rbinom(length(UnterHR$LeaveJob[UnterHR$Treatment==0 & UnterHR$Female==0]),1,.3)
    UnterHR$LeaveJob[UnterHR$Treatment==1 & UnterHR$Female==0]<-rbinom(length(UnterHR$LeaveJob[UnterHR$Treatment==1 & UnterHR$Female==0]),1,.2)
  #treatment makes wommen more likely to leave
    UnterHR$LeaveJob[UnterHR$Treatment==0 & UnterHR$Female==1]<-rbinom(length(UnterHR$LeaveJob[UnterHR$Treatment==0 & UnterHR$Female==1]),1,.3)
    UnterHR$LeaveJob[UnterHR$Treatment==1 & UnterHR$Female==1]<-rbinom(length(UnterHR$LeaveJob[UnterHR$Treatment==1 & UnterHR$Female==1]),1,.6)
  #treatment makes blacks and latinos more likely to leave (redraw if leave job = 0 and and black or latino = 1)
    #count people in either category
      n2<-length(UnterHR$LeaveJob[UnterHR$Treatment==1 & UnterHR$LeaveJob==0 & (UnterHR$Race=="Black" | UnterHR$Race=="Latino" )])
    #redraw for those
      UnterHR$LeaveJob[UnterHR$Treatment==1 & UnterHR$LeaveJob==0 & (UnterHR$Race=="Black" | UnterHR$Race=="Latino" )]<-rbinom(n2,1,.5)
```

`@sample_code`
```{r}
# 1) First, let's construct a naive regression model that just uses the effect of `Treatment` and `Female` on `LeaveJob` in the dataframe `UnterHR`. We recommend using the summary function around a glm function of this relationship.

    summary(glm( ))

# Note: The model you just created does not assume that the treatment effects women differently, but that women have a different baseline chance of leaving their job, regardless of the treatment effect. From the output above, it seems that being a woman increases one's likelihood of leaving Unter, but the treatment does not. 

# 2) In order to find out if being a women moderates the the treatment effect (i.e. the treatment influences men and women differently), we need to add an interaction between Female and Treatment. Construct a regression model that measures the effect of `Treatment` on `LeaveJob`, with a moderation effect for `Female`. For reference, the syntax for glm with an interaction effect is "glm(Y~X*Z,data=df)". The difference between the syntax for a mediation and moderation effect is that we multiply X and Z for moderation, rather than add them. 
 


# Note: By default, R includes independent variables for Treatment and Female when we include a statistical interaction term. The coefficient for Treatment indicates the effect of Treatment for non-women (men); the coefficient for Female indicates the effect of being a female, independent of Treatment. The Treatment:Female coefficient indicates the effect of Treatment for Females. Now the coefficients for Treatment and Female are no longer statistically significant. There is a large coefficient for the interaction term, suggests that the Treatment has a large effect on women's odds of leaving their job.
      
# 3) Let's now answer the final question for this exercise. Create a glm model that includes an interaction effect between Treatment and Female, and an interaction effect between Treatment and Race. The syntax is much like above, but now we add a second pair of variables that are interacting (e.g. "glm(Y~X*Z+W*Z,data=df)").

```

`@solution`
```{r}
summary(glm(LeaveJob ~ Treatment + Female,data=UnterHR))
summary(glm(LeaveJob ~ Treatment * Female,data=UnterHR))
summary(glm(LeaveJob ~ Treatment * Female + Treatment * Race,data=UnterHR))
```

`@sct`
```{r}
test_error()
success_msg("Good work! It appears that the treatment only affected individuals who were Female, Black, or Latino. The estimates for the non-significant effects are unimportant")
```

---

## Logistic Regression Models

```yaml
type: VideoExercise
key: 7fb39b7cb6
lang: r
xp: 50
skills: 1
video_link: //player.vimeo.com/video/233015652
```


---

## When to Use a Logistic Regression Model

```yaml
type: MultipleChoiceExercise
key: 3e6268810e
lang: r
xp: 50
skills: 1
```

For which of the following causal relationships might we prefer to model with logistic regression rather than OLS regression?

`@instructions`
- The effect of eating ice cream on a person's likelihood of being attacked by a seagull.
- The effect of divorce on people's margarine consumption.
- The effect of snowfall on ski resort profits
- The effect of global temperatures on the number of pirate attacks across the world

`@sct`
```{r}
msg1 = "Correct! The dependent variable in this question is binary; a person either has or hasn't ever been attacked by a seagull. This is when logistic regression models are most useful."
msg2 = "Almost. In this example, the independent variable is binary, but the dependent variable is continuous. Try again."
msg3 = "The dependent variable in this causal relationship is continuous, therefore it would be impossible to model this relationship with logistic regression. Try again"
msg4 = "The dependent variable in this causal relationship is continuous, therefore it would be impossible to model this relationship with logistic regression. Try again"
test_mc(correct = 1, feedback_msgs = c(msg1,msg2,msg3,msg4))
```

---

## Defining The Average Effect of Treatment on the Treated

```yaml
type: VideoExercise
key: a9f7023dbf
lang: r
xp: 50
skills: 1
video_link: //player.vimeo.com/video/217554577
```


---

## Average Effect of Treatment on the Treated

```yaml
type: MultipleChoiceExercise
key: 05b8c0ab9e
lang: r
xp: 50
skills: 1
```

A scientist develops a new drug to help reduce addiction to pain killers. He administers this drug to many recent abusers of painkillers, and finds that these patients had substantially lower rates of painkiller abuse than would be expected, given that painkiller addicts often relapse following rehab. However, across the **entire human population**, the scientist expects the average treatment effect of his drug to be negligible on rates of painkiller abuse. Why might the scientist expect the average treatment effect on the treated to be larger than the average treatment effect?

`@instructions`
- Because those who were treated with the drug were more likely to gain benefit from using it.
- Because painkiller abusers are an unreliable sample. 
- Because the sample who tried the scientist's drug was small.

`@sct`
```{r}
msg1 = "Correct! The only people who were administered the drug were painkiller addicts, but this is a relatively small portion of the population. The drug was designed to reduce addiction to painkillers, so cannot help the majority of individuals in the population who are not addicted to pain killers"
msg2 = "Not quite. Think through why we care about ATT and try again"
msg3 = "Actually, unless we know that a study's sample was biased, we have no reason to expect the observed treatment effect to increase or decrease. Try again."
test_mc(correct = 1, feedback_msgs = c(msg1,msg2,msg3))
```

---

## How to Compute ATE Under Unconfoundedness, and What Not to Do

```yaml
type: VideoExercise
key: 74764898
lang: r
xp: 50
skills: 1
video_link: //player.vimeo.com/video/217554781
```


---

## Practice with Survey Weights: Part 1

```yaml
type: NormalExercise
key: 671450a209
lang: r
xp: 100
skills: 1
```

Given the diversity of people in the United States, many surveys have difficulty gathering perfectly representative samples of the population. This can be problematic when your research results are meant to be generalizable to the population at large. However, surveyors typically know the true proportion of demographic traits among the U.S. population, and provide survey weights that compensates for their sampling bias, by increasing the importance of underrepresented groups in individual's statistical analyses. In this problem, we will practice using survey weights.

The American film studio, Delimited Pictures, is interested in how many tickets they should expect to sell to adults for their upcoming film, Cambrian Park, and what age group are most likely to watch it. They interview people across the country, asking them whether they plan to see the movie when it comes out. Although their resulting sample was not perfectly representative of the country's adult population, a statistician was able to provide them with survey weights that compensate for this sampling error. Use these survey weights and the dataset `Survey` to help Delimited Pictures estimate what proportion of the U.S. adult population will see their upcoming movie, and to determine what age group is most likely to watch it.

`@instructions`
- 1) Get a sense of survey weights by summarizing the dataframe `Survey`.
- 2) Look through the following example of how survey weights can compensate for unrepresentative sampling.
- 3) Estimate the proportion of the US adult population that plans to watch Cambrian Park (variable `WillWatch`) while using the provided survey weights.

`@pre_exercise_code`
```{r}
set.seed(1)
n=1775
#Create Dataframe
  Survey<-data.frame(Black=rbinom(n,1,.1))
  Survey$Latino<-ifelse(Survey$Black==1,0,rbinom(n-sum(Survey$Black),1,.07))
  Survey$White<-ifelse(Survey$Black==1 | Survey$Latino==1,0,rbinom(n-sum(Survey$Black)-sum(Survey$Latino),1,.95))
  Survey$Other<-ifelse(Survey$Black==1 | Survey$Latino==1 | Survey$White==1,0,1)
  Survey$Female<-rbinom(n,1,.6)
  Survey$Age<-round(c(rnorm(.7*n+1,mean=33,sd=4),rnorm(.1*n,mean=45,sd=5),rnorm(.2*n,mean=55,sd=5)))
  Survey$Weight<-1.35*Survey$Black+1*Survey$White+1.3*Survey$Latino+1.07*Survey$Other+Survey$Age*.005+.3*(1-Survey$Female)-.3699
  Survey$WillWatch<-rbinom(n,1,.01+.03*(1-Survey$Female)+.01*Survey$Black+.00003*Survey$Age^2)
```

`@sample_code`
```{r}
# 1) To get used to what survey weights like look like, let's summarize the `Weight` variable in our dataset `Survey` with the summary() command:


    
# Note: The weights range between 0.7 and 1.6 in this dataset, but you may encounter survey datasets with much larger weights. In general, because the mean weight of 1 is closer to 0.7 than it is to 1.6, we can tell that individuals with higher weights were undersampled. As you might guess, observations with higher weights have more influence in your statistical models. 

    
# 2) As a teaser to what we are doing when we "weight" the data, let's look at the variable for gender, 'Female`. The first line below shows the proportion of females in this survey. Select the following code and hit the "Run Code" button to see the results.

    prop.table(table(Survey$Female))

# Note: This number is much too uneven to be representative of the actual US population. The following line indicates the proportion of females in these survey when they are weighted. We have generated the code for you again, so select it and hit the "Run Code" button to see the results.

    prop.table(xtabs(Weight~Female, data=Survey))

# Note: These numbers appear much more realistic. There are about an equal number of men and women in the population.   


# 3) But before we weigh the surveys, let's see what the unweighted data says is the proportion of the U.S. population that is planning to watch Cambrian Park. This is simple: just find the mean value of the variable `WillWatch`:

    mean()
```

`@solution`
```{r}
summary(Survey$Weight)
mean(Survey$WillWatch)
```

`@sct`
```{r}
test_error()
success_msg("Good work, but our sample was a bit skewed, so we can't trust this number. On top of that, DeLimited Pictures expected that at least 7% of the population would watch their film, so we're under pressure. Let's try using the data to weigh the survey responses for a more realistic answer.")
```

---

## Practice with Survey Weights: Part 2

```yaml
type: NormalExercise
key: 9ac73c8105
lang: r
xp: 100
skills: 1
```

Given the diversity of people in the United States, many surveys have difficulty gathering perfectly representative samples of the population. This can be problematic when your research results are meant to be generalizable to the population at large. However, surveyors typically know the true proportion of demographic traits among the U.S. population, and provide survey weights that compensates for their sampling bias, by increasing the importance of underrepresented groups in individual's statistical analyses. In this problem, we will practice using survey weights.

The American film studio, Delimited Pictures, is interested in how many tickets they should expect to sell to adults for their upcoming film, Cambrian Park, and what age group are most likely to watch it. They interview people across the country, asking them whether they plan to see the movie when it comes out. Although their resulting sample was not perfectly representative of the country's adult population, a statistician was able to provide them with survey weights that compensate for this sampling error. Use these survey weights and the dataset `Survey` to help Delimited Pictures estimate what proportion of the U.S. adult population will see their upcoming movie, and to determine what age group is most likely to watch it.

`@instructions`
- 1) Estimate the proportion of the US adult population that plans to watch Cambrian Park (variable `WillWatch`) while using the provided survey weights.
- 2) Construct a survey-weighted OLS regression model to estimate the effect of `Age` on `WillWatch` controlling for `Female`.

`@pre_exercise_code`
```{r}
set.seed(1)
n=1775
#Create Dataframe
  Survey<-data.frame(Black=rbinom(n,1,.1))
  Survey$Latino<-ifelse(Survey$Black==1,0,rbinom(n-sum(Survey$Black),1,.07))
  Survey$White<-ifelse(Survey$Black==1 | Survey$Latino==1,0,rbinom(n-sum(Survey$Black)-sum(Survey$Latino),1,.95))
  Survey$Other<-ifelse(Survey$Black==1 | Survey$Latino==1 | Survey$White==1,0,1)
  Survey$Female<-rbinom(n,1,.6)
  Survey$Age<-round(c(rnorm(.7*n+1,mean=33,sd=4),rnorm(.1*n,mean=45,sd=5),rnorm(.2*n,mean=55,sd=5)))
  Survey$Weight<-1.35*Survey$Black+1*Survey$White+1.3*Survey$Latino+1.07*Survey$Other+Survey$Age*.005+.3*(1-Survey$Female)-.3699
  Survey$WillWatch<-rbinom(n,1,.01+.03*(1-Survey$Female)+.01*Survey$Black+.00003*Survey$Age^2)
```

`@sample_code`
```{r}
# 1) What about if we use the survey weights to get a more representative estimation of the proportion of people who will watch the film? Try to compute the weighted mean for WillWatch. The syntax for determining the weighted mean of a variable is the following: weighted.mean([VariableOfInterest],[WeightVariable]). 
    
    weighted.mean()
    
# Note: It looks like the original sample underrepresented the proportion of people who might actually watch Cambrian Park. The weighted mean is above 7%! 

# 2) Let's now construct a survey-weighted OLS regression model that estimates the effect of Age on WillWatch, controlling for Female. The syntax is almost identical to normal regression models. We simply need to add a statement to identify survey weights. The syntax is `glm(Y ~ X + Z, weight = [Weight], data = [dataframe])`, where Y = the name of the dependent variable, X = the name of the independent variable, Z = the name of the control variable Weight = the weight variable, and dataframe = the dataframe that contains these variables.

    summary(glm())
```

`@solution`
```{r}
weighted.mean(Survey$WillWatch,Survey$Weight)
summary(glm(WillWatch ~ Female+Age, weight=Weight, data=Survey))
```

`@sct`
```{r}
test_error()
success_msg("Good work! It seems that women are 5% less likely to go to see Cambrian park than men, and older  (AKA dinosaurs), are more likely to see Cambrian Park. Although survey weights do not often change one's results substantially, it is still good practice to use them if they are provided to you, especially if you are trying to study trends about an entire population. If you are doing extensive analysis with weighted data, you may want to consider using the 'survey' package instead of the weight options in R's built in functions.")
```

---

## When Survey Weights Are Unnecessary

```yaml
type: MultipleChoiceExercise
key: 48a5526aa9
lang: r
xp: 50
skills: 1
```

One week after Cambrian Park was released, Delimited Pictures gathered a survey of people who watched it and asked them whether or not they liked the film. Delimited Pictures put significant effort into catering to a younger female audience - individuals who had the least positive reaction to Cambrian Park's prequel, Neoproterozoic Park. If Delimited Pictures is just interested in whether female college students in Los Angeles seemed to enjoy Cambrian Park, do they need to use survey weights to poststratify their sample?

`@instructions`
- Yes
- No

`@sct`
```{r}
msg1 = "Try again"
msg2 = "Correct! Survey weights are only useful when you are trying to generalize a finding to the general population. They are typically only used for analysis of national trends. Since Delimited Pictures is only interested in how a narrow demographic perceived their new film, it is unlikely that they would need to weight their data."
test_mc(correct = 2, feedback_msgs = c(msg1,msg2))
```
